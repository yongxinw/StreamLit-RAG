import openai
import streamlit as st
from llama_index import (
    Document,
    ServiceContext,
    SimpleDirectoryReader,
    VectorStoreIndex,
)
from llama_index.agent import ReActAgent
from llama_index.llms import OpenAI
from llama_index.tools import BaseTool, FunctionTool, QueryEngineTool, ToolMetadata
from llama_index.prompts import PromptTemplate
from statics import REGISTRATION_STATUS

def display_prompt_dict(prompts_dict):
    for k, p in prompts_dict.items():
        text_md = f"**Prompt Key**: {k}\n" f"**Text:** \n"
        print(text_md)
        print(p.get_template())

st.set_page_config(page_title="Chat with the Streamlit docs, powered by LlamaIndex", page_icon="ğŸ¦™", layout="centered", initial_sidebar_state="auto", menu_items=None)
openai.api_key = st.secrets.openai_key
# openai.api_key = "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"
st.title("Chat with the Streamlit docs, powered by LlamaIndex ğŸ’¬ğŸ¦™")
st.info("Check out the full tutorial to build this app in our [blog post](https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/)", icon="ğŸ“ƒ")

if "messages" not in st.session_state.keys():  # Initialize the chat messages history
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "Ask me a question about Streamlit's open-source Python library!",
        }
    ]

agent_template_str = (
    '\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses. \n\n'
    '## Tools\nYou have access to a wide variety of tools. You are responsible for using\n'
    'the tools in any sequence you deem appropriate to complete the task at hand.\n'
    'This may require breaking the task into subtasks and using different tools\n'
    'to complete each subtask.\n\n'
    'You have access to the following tools:\n{tool_desc}\n\n## Output Format\n'
    'To answer the question, please use the following format.\n\n'
    '```\nThought: I need to use a tool to help me answer the question.\n'
    'Action: tool name (one of {tool_names}) if using a tool.\n'
    'Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{"input": "hello world", "num_beams": 5}})\n'
    '```\n\nPlease ALWAYS start with a Thought.\n\n'
    'Please use a valid JSON format for the Action Input. Do NOT do this {{\'input\': \'hello world\', \'num_beams\': 5}}.\n\n'
    'If this format is used, the user will respond in the following format:\n\n'
    '```\nObservation: tool response\n```\n\n'
    'You should keep repeating the above format until you have enough information\n'
    'to answer the question without using any more tools. At that point, you MUST respond\n'
    'in the one of the following two formats:\n\n```\n'
    'Thought: I can answer without using any more tools.\n'
    'Answer: [your answer here]\n```\n\n```\n'
    'Thought: I cannot answer the question with the provided tools.\n'
    'Answer: Sorry, I cannot answer your query.\n```\n\n'
    'ALWAYS check user role from chat history before any actions.\n'
    'When user role is unknown, you MUST ask the user for his role based on policy engine output and MUST NOT use any tools to infer user role or ask directly.'
    'When user has provided role information, use the correct tool to update user role and proceed with the answering questions.\n'
    'Current user role is unknown\n\n'
    '## Current Conversation\n'
    'Below is the current conversation consisting of interleaving human and assistant messages.\n\n'
)
policy_engine_tmpl_str = (
    "æ³¨æ„ï¼šå›ç­”é—®é¢˜å‰ï¼Œè¯·å…ˆä»ç”¨æˆ·å¯¹è¯ä¸­å°è¯•ç¡®å®šç”¨æˆ·è§’è‰²ï¼Œè‹¥æ— æ³•æ¨æµ‹ï¼Œåˆ™è¯¢é—®ç”¨æˆ·è§’è‰²ï¼Œä¸è¦æ¨æµ‹ç”¨æˆ·è§’è‰²ï¼Œå›ç­”é—®é¢˜æ—¶è¯·ä¿æŒæŠ€æœ¯æ€§å’ŒåŸºäºäº‹å®ï¼Œä¸è¦äº§ç”Ÿå¹»è§‰ã€‚\n"
    # "æ³¨æ„ï¼šè‹¥ç”¨æˆ·è§’è‰²ä¸ºæœªçŸ¥ï¼Œè¯·å…ˆè¯¢é—®ç”¨æˆ·è§’è‰²ï¼Œä¸è¦æ¨æµ‹ç”¨æˆ·è§’è‰²ï¼Œå›ç­”é—®é¢˜æ—¶è¯·ä¿æŒæŠ€æœ¯æ€§å’ŒåŸºäºäº‹å®ï¼Œä¸è¦äº§ç”Ÿå¹»è§‰ã€‚\n"
    "æ³¨æ„ï¼šç”¨æˆ·è§’è‰²ä¸ºæœªçŸ¥\n"
    "è¯­å¢ƒä¿¡æ¯å¦‚ä¸‹\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "è¯·æ ¹æ®è¯­å¢ƒä¿¡æ¯ï¼Œä¸è¦ä½¿ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå›ç­”ä¸‹é¢çš„é—®é¢˜ã€‚\n"
    "Query: {query_str}\n"
    "Answer: "
)
policy_engine_refine_tmpl_str = (
    "ä»¥ä¸‹æ˜¯åŸå§‹æŸ¥è¯¢ï¼š{query_str}\n"
    "æˆ‘ä»¬å·²ç»æä¾›äº†ä¸€ä¸ªç°æœ‰çš„ç­”æ¡ˆï¼š{existing_answer}\n"
    "æˆ‘ä»¬æœ‰æœºä¼šé€šè¿‡ä¸‹é¢çš„ä¸€äº›æ›´å¤šä¸Šä¸‹æ–‡æ¥æ”¹è¿›ç°æœ‰çš„ç­”æ¡ˆï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰ã€‚\n"
    "------------\n"
    "{context_msg}"
    "------------\n"
    "æ ¹æ®æ–°çš„ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›åŸå§‹ç­”æ¡ˆä»¥æ›´å¥½åœ°å›ç­”æŸ¥è¯¢ã€‚å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç”¨ï¼Œè¿”å›åŸå§‹ç­”æ¡ˆã€‚\n"
    "Refined Answer:"
)


def display_prompt_dict(prompts_dict):
    for k, p in prompts_dict.items():
        text_md = f"**Prompt Key**: {k}\n" f"**Text:** \n"
        print(text_md)
        print(p.get_template())


openai.api_key = st.secrets.openai_key

llm = OpenAI(
    model="gpt-4-turbo-preview",
    # model="gpt-3.5-turbo-0613",
    # model="gpt-4-0613",
    # model="gpt-4-0125-preview",
    temperature=0.4,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå…³äºå¤§ä¼—äº‘å­¦çš„ä¸“å®¶ï¼Œä½ äº†è§£å…³äºå¤§ä¼—äº‘å­¦çš„æ‰€æœ‰é—®é¢˜ã€‚ç”¨æˆ·è§’è‰²æœªçŸ¥æ—¶ï¼Œè¯·å…ˆè¯¢é—®è§’è‰²ã€‚å‡è®¾æ‰€æœ‰çš„é—®é¢˜éƒ½ä¸å¤§ä¼—äº‘å­¦æœ‰å…³ã€‚ä¿æŒä½ çš„ç­”æ¡ˆæŠ€æœ¯æ€§å’ŒåŸºäºäº‹å®â€”â€”ä¸è¦äº§ç”Ÿå¹»è§‰ã€‚",
)

@st.cache_data()
def load_data():
    reader = SimpleDirectoryReader(input_dir="./policies", recursive=True)
    docs = reader.load_data()
    service_context = ServiceContext.from_defaults(llm=llm)
    index = VectorStoreIndex.from_documents(docs, service_context=service_context)
    return index


index = load_data()


def multiply(a: int, b: int) -> int:
    """è®¡ç®—ä¸¤ä¸ªæ•°çš„ä¹˜ç§¯å¹¶è¿”å›ç»“æœ"""
    return a * b


def update_user_role(input: str = "123"):
    """
    æ ¹æ®è¯­å¢ƒï¼Œæ›´æ–°ç”¨æˆ·è§’è‰²ä»¥åŠå¯¹åº”çš„å›ç­”æ¨¡æ¿ï¼Œä»¥ä¾¿æ›´å¥½åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    """
    # TODO: This is a hacky fix. A better solution: https://stackoverflow.com/questions/11283961/partial-string-formatting
    USER_ROLE = input
    policy_engine_tmpl_str = (
        "æ³¨æ„ï¼šå›ç­”é—®é¢˜å‰ï¼Œè¯·å…ˆä»ç”¨æˆ·å¯¹è¯ä¸­å°è¯•ç¡®å®šç”¨æˆ·è§’è‰²ï¼Œè‹¥æ— æ³•æ¨æµ‹ï¼Œåˆ™è¯¢é—®ç”¨æˆ·è§’è‰²ï¼Œä¸è¦æ¨æµ‹ç”¨æˆ·è§’è‰²ï¼Œå›ç­”é—®é¢˜æ—¶è¯·ä¿æŒæŠ€æœ¯æ€§å’ŒåŸºäºäº‹å®ï¼Œä¸è¦äº§ç”Ÿå¹»è§‰ã€‚\n"
        #  "æ³¨æ„ï¼šè‹¥ç”¨æˆ·è§’è‰²ä¸ºæœªçŸ¥ï¼Œè¯·å…ˆè¯¢é—®ç”¨æˆ·è§’è‰²ï¼Œä¸è¦æ¨æµ‹ç”¨æˆ·è§’è‰²ï¼Œå›ç­”é—®é¢˜æ—¶è¯·ä¿æŒæŠ€æœ¯æ€§å’ŒåŸºäºäº‹å®ï¼Œä¸è¦äº§ç”Ÿå¹»è§‰ã€‚\n"
        "æ³¨æ„ï¼šç”¨æˆ·è§’è‰²ä¸º{user_role}\n"
        "è¯­å¢ƒä¿¡æ¯å¦‚ä¸‹\n"
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        "è¯·æ ¹æ®è¯­å¢ƒä¿¡æ¯ï¼Œä¸è¦ä½¿ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå›ç­”ä¸‹é¢çš„é—®é¢˜ã€‚\n"
        "Query: {query_str}\n"
        "Answer: "
    )
    print(USER_ROLE)
    template_str = policy_engine_tmpl_str.format(
        context_str="{context_str}", user_role=USER_ROLE if USER_ROLE is not None else "æœªçŸ¥", query_str="{query_str}"
    )
    print(template_str)
    policy_engine_tmpl = PromptTemplate(template_str)
    agent.agent_worker._get_tools("")[0]._query_engine.update_prompts(
        {"response_synthesizer:text_qa_template": policy_engine_tmpl}
    )

    agent_template_str = (
        '\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses. \n\n'
        '## Tools\nYou have access to a wide variety of tools. You are responsible for using\n'
        'the tools in any sequence you deem appropriate to complete the task at hand.\n'
        'This may require breaking the task into subtasks and using different tools\n'
        'to complete each subtask.\n\n'
        'You have access to the following tools:\n{tool_desc}\n\n## Output Format\n'
        'To answer the question, please use the following format.\n\n'
        '```\nThought: I need to use a tool to help me answer the question.\n'
        'Action: tool name (one of {tool_names}) if using a tool.\n'
        'Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{"input": "hello world", "num_beams": 5}})\n'
        '```\n\nPlease ALWAYS start with a Thought.\n\n'
        'Please use a valid JSON format for the Action Input. Do NOT do this {{\'input\': \'hello world\', \'num_beams\': 5}}.\n\n'
        'If this format is used, the user will respond in the following format:\n\n'
        '```\nObservation: tool response\n```\n\n'
        'You should keep repeating the above format until you have enough information\n'
        'to answer the question without using any more tools. At that point, you MUST respond\n'
        'in the one of the following two formats:\n\n```\n'
        'Thought: I can answer without using any more tools.\n'
        'Answer: [your answer here]\n```\n\n```\n'
        'Thought: I cannot answer the question with the provided tools.\n'
        'Answer: Sorry, I cannot answer your query.\n```\n\n'
        'ALWAYS check user role from chat history before any actions.\n'
        'When user role is unknown, you MUST ask the user for his role based on policy engine output and MUST NOT use any tools to infer user role or ask directly.'
        'When user has provided role information, use the correct tool to update user role and proceed with the answering questions.\n'
        'Current user role is' + USER_ROLE + '\n\n'
        'If the user is unsure of whether they have registered, you MUST ask them to provide the administrator ID number and THEN use the right tool to check the registration status.\n\n'
        '## IMPORTANT: \n'
        'All conversation is in Chinese. Please use Chinese for all conversation.\n\n'
        '## Current Conversation\n'
        'Below is the current conversation consisting of interleaving human and assistant messages.\n\n'
    )
    agent.update_prompts({"agent_worker:system_prompt": PromptTemplate(agent_template_str)})
    return "user role updated"

def lookup_by_id(input: str = "123"):
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥çš„IDå·ï¼ŒæŸ¥è¯¢ç”¨æˆ·åœ¨å¤§ä¼—äº‘å­¦å¹³å°ä¸Šçš„æ³¨å†ŒçŠ¶æ€ã€‚
    """
    if REGISTRATION_STATUS.get(input) is not None:
        return REGISTRATION_STATUS.get(input)
    return "ç»æŸ¥è¯¢ï¼Œæ‚¨å°šæœªåœ¨å¤§ä¼—äº‘å­¦å¹³å°ä¸Šæ³¨å†Œ"

multiply_tool = FunctionTool.from_defaults(
    fn=multiply,
    tool_metadata=ToolMetadata(name="multiply", description="è®¡ç®—ä¸¤ä¸ªæ•°çš„ä¹˜ç§¯å¹¶è¿”å›ç»“æœã€‚"),
)

update_user_role_tool = FunctionTool.from_defaults(
    fn=update_user_role,
    tool_metadata=ToolMetadata(
        name="update_role", description="æ ¹æ®è¯­å¢ƒï¼Œæ›´æ–°ç”¨æˆ·è§’è‰²ä»¥åŠå¯¹åº”çš„å›ç­”æ¨¡æ¿ï¼Œä»¥ä¾¿æ›´å¥½åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
    ),
)

lookup_by_id_tool = FunctionTool.from_defaults(
    fn=lookup_by_id,
    tool_metadata=ToolMetadata(
        name="lookup_by_id",
        description="æ ¹æ®ç”¨æˆ·è¾“å…¥çš„IDå·ï¼ŒæŸ¥è¯¢ç”¨æˆ·åœ¨å¤§ä¼—äº‘å­¦å¹³å°ä¸Šçš„æ³¨å†ŒçŠ¶æ€ã€‚",
    ),
)

policy_engine_tmpl = PromptTemplate(policy_engine_tmpl_str)
policy_enging_refine_tmpl = PromptTemplate(policy_engine_refine_tmpl_str)
policy_engine = index.as_query_engine(
    similarity_top_k=5,
    verbose=True,
)
policy_engine.update_prompts(
    {"response_synthesizer:text_qa_template": policy_engine_tmpl}
)
policy_engine.update_prompts(
    {"response_synthesizer:refine_template": policy_enging_refine_tmpl}
)
policy_query_tool = QueryEngineTool(
    query_engine=policy_engine,
    metadata=ToolMetadata(
        name="policy_engine",
        description="æŸ¥è¯¢å¤§ä¼—äº‘å­¦ä½¿ç”¨æ¡æ¬¾åŠæ–¹æ³•ï¼Œå…·ä½“å…³äºå¦‚ä½•æ³¨å†Œå’Œå¦‚ä½•æŸ¥è¯¢è¯ä¹¦å’Œå­¦æ—¶ç­‰é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚",
    ),
)

tools = [
    policy_query_tool,
    # multiply_tool,
    update_user_role_tool,
    lookup_by_id_tool
]

agent = ReActAgent.from_tools(tools=tools, llm=llm, verbose=True)
agent.update_prompts({"agent_worker:system_prompt": PromptTemplate(agent_template_str)})
# response = agent.chat("ä½ å¥½ï¼Œæˆ‘æƒ³çŸ¥é“å¤§ä¼—äº‘å­¦çš„ä½¿ç”¨æ¡æ¬¾åŠæ–¹æ³•ã€‚")
# response = agent.chat("155ä¹˜ä»¥203ç­‰äºå¤šå°‘")
# print(str(response))
# exit()
if "chat_engine" not in st.session_state.keys():  # Initialize the chat engine
    # st.session_state.chat_engine = index.as_chat_engine(
    #     chat_mode="condense_question", verbose=True
    # )
    st.session_state.chat_engine = agent

if prompt := st.chat_input(
    "Your question"
):  # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # st.session_state.chat_engine.memory.add_message(
    #     {"role": "user", "content": prompt}
    # )

for message in st.session_state.messages:  # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = st.session_state.chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            # st.session_state.chat_engine.memory.add_message(message)
            st.session_state.messages.append(message)  # Add response to message history
