import json
import os
import sys
from typing import Any, List, Optional, Type

import streamlit as st
from langchain import hub
from langchain.agents import AgentExecutor, create_react_agent
from langchain.callbacks.manager import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from langchain.chains import LLMChain
from langchain.embeddings.dashscope import DashScopeEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.pydantic_v1 import BaseModel, Field
from langchain.retrievers import ContextualCompressionRetriever
from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
)

# from langchain_core.tools import BaseTool
from langchain.tools import BaseTool, StructuredTool, tool
from langchain.tools.retriever import create_retriever_tool
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_community.llms import Tongyi
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableBranch, RunnableLambda

from statics import COURSE_PURCHASES, CREDIT_HOURS, LOC_STR, REGISTRATION_STATUS
import re

# from langchain_openai import ChatOpenAI, OpenAIEmbeddings


os.environ["DASHSCOPE_API_KEY"] = "sk-91ee79b5f5cd4838a3f1747b4ff0e850"
# os.environ["DASHSCOPE_API_KEY"] = "sk-c92ed98926194b84a41a73db62af31d5"
os.environ["OPENAI_API_KEY"] = "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"
st.set_page_config(
    page_title="å¤§ä¼—äº‘å­¦æ™ºèƒ½å®¢æœå¹³å°",
    page_icon="ğŸ¦™",
    layout="centered",
    initial_sidebar_state="auto",
    menu_items=None,
)
st.title("å¤§ä¼—äº‘å­¦æ™ºèƒ½å®¢æœå¹³å°, powered by LangChain")
if "messages" not in st.session_state.keys():  # Initialize the chat messages history
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "æ¬¢è¿æ‚¨æ¥åˆ°å¤§ä¼—äº‘å­¦ï¼Œæˆ‘æ˜¯å¤§ä¼—äº‘å­¦çš„ä¸“å®¶åŠ©æ‰‹ï¼Œæˆ‘å¯ä»¥å›ç­”å…³äºå¤§ä¼—äº‘å­¦çš„æ‰€æœ‰é—®é¢˜ã€‚æµ‹è¯•è¯·ä½¿ç”¨èº«ä»½è¯å·372323199509260348ã€‚æµ‹è¯•å…¬éœ€è¯¾/ä¸“ä¸šè¯¾å­¦æ—¶ï¼Œè¯·ä½¿ç”¨å¹´ä»½2019/2020ã€‚æµ‹è¯•è¯¾ç¨‹è´­ä¹°ï¼Œé€€æ¬¾ç­‰ï¼Œè¯·ä½¿ç”¨å¹´ä»½2023ï¼Œè¯¾ç¨‹åç§°æ–°é—»ä¸“ä¸šè¯¾åŸ¹è®­ç­ã€‚",
        }
    ]


# Simple demo tool - a simple calculator
class SimpleCalculatorTool(BaseTool):
    """è®¡ç®—ä¸¤ä¸ªè¾“çš„ä¹˜ç§¯çš„ç®€å•è®¡ç®—å™¨"""

    name: str = "ç®€å•è®¡ç®—å™¨"
    description: str = (
        "ç”¨äºè®¡ç®—ä¸¤ä¸ªæ•°çš„ä¹˜ç§¯ï¼Œéœ€è¦æŒ‡é€šè¿‡ json æŒ‡å®šç¬¬ä¸€ä¸ªæ•° first_numberã€ç¬¬äºŒä¸ªæ•° second_number "
    )
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    # def _run(self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None) -> Any:
    def _run(self, params) -> Any:
        print(params)
        # print(type(params))
        params_dict = json.loads(params)
        return params_dict["first_number"] * params_dict["second_number"]


class RegistrationStatusTool(BaseTool):
    """æŸ¥è¯¢ç”¨æˆ·åœ¨å¤§ä¼—äº‘å­¦å¹³å°ä¸Šçš„æ³¨å†ŒçŠ¶æ€"""

    name: str = "æ³¨å†ŒçŠ¶æ€æŸ¥è¯¢å·¥å…·"
    description: str = (
        "ç”¨äºæŸ¥è¯¢ç”¨æˆ·åœ¨å¤§ä¼—äº‘å­¦å¹³å°ä¸Šçš„æ³¨å†ŒçŠ¶æ€ï¼Œéœ€è¦æŒ‡é€šè¿‡ json æŒ‡å®šç”¨æˆ·èº«ä»½è¯å· user_id_number "
    )
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    # def _run(self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None) -> Any:
    def _run(self, params) -> Any:
        print(params)
        # print(type(params))
        try:
            params_dict = json.loads(params)
        except json.JSONDecodeError:
            return "è¯·æŒ‡å®šæ‚¨æˆ–è€…ç®¡ç†å‘˜èº«ä»½è¯å·"
        if "user_id_number" not in params_dict:
            return "è¯·æŒ‡å®šæ‚¨æˆ–è€…ç®¡ç†å‘˜èº«ä»½è¯å·"
        try:
            int(params_dict["user_id_number"])
        except ValueError:
            return "è¯·æŒ‡å®šæ‚¨æˆ–è€…ç®¡ç†å‘˜èº«ä»½è¯å·"
        input = params_dict["user_id_number"]
        if REGISTRATION_STATUS.get(input) is not None:
            status = REGISTRATION_STATUS.get(input)
            ret_str = [f"{k}: {v}" for k, v in status.items()]
            ret_str = "  \n".join(ret_str)
            return "ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨å¤§ä¼—äº‘å­¦å¹³å°ä¸Šçš„æ³¨å†ŒçŠ¶æ€å¦‚ä¸‹ï¼š  \n" + ret_str
        return "ç»æŸ¥è¯¢ï¼Œæ‚¨å°šæœªåœ¨å¤§ä¼—äº‘å­¦å¹³å°ä¸Šæ³¨å†Œ"


class UpdateUserRoleTool(BaseTool):
    """æ ¹æ®ç”¨æˆ·å›ç­”ï¼Œæ›´æ–°ç”¨æˆ·è§’è‰²"""

    name: str = "ç”¨æˆ·è§’è‰²æ›´æ–°å·¥å…·"
    description: str = (
        "ç”¨äºæ›´æ–°ç”¨æˆ·åœ¨å¯¹è¯ä¸­çš„è§’è‰²ï¼Œéœ€è¦æŒ‡é€šè¿‡ json æŒ‡å®šç”¨æˆ·è§’è‰² user_role "
    )
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    # def _run(self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None) -> Any:
    def _run(self, params) -> Any:
        print(params)
        try:
            params_dict = json.loads(params)
        except json.JSONDecodeError:
            return "æ‚¨å¥½ï¼Œç›®å‰æˆ‘ä»¬æ”¯æŒçš„ç”¨æˆ·ç±»å‹ä¸ºä¸“æŠ€ä¸ªäººï¼Œç”¨äººå•ä½ï¼Œä¸»ç®¡éƒ¨é—¨å’Œç»§ç»­æ•™è‚²æœºæ„ï¼Œè¯·ç¡®è®¤æ‚¨çš„ç”¨æˆ·ç±»å‹ã€‚"
        if "user_role" not in params_dict:
            return "æ‚¨å¥½ï¼Œç›®å‰æˆ‘ä»¬æ”¯æŒçš„ç”¨æˆ·ç±»å‹ä¸ºä¸“æŠ€ä¸ªäººï¼Œç”¨äººå•ä½ï¼Œä¸»ç®¡éƒ¨é—¨å’Œç»§ç»­æ•™è‚²æœºæ„ï¼Œè¯·ç¡®è®¤æ‚¨çš„ç”¨æˆ·ç±»å‹ã€‚"
        user_role = params_dict["user_role"]
        if user_role not in ["ä¸“æŠ€ä¸ªäºº", "ç”¨äººå•ä½", "ä¸»ç®¡éƒ¨é—¨", "ç»§ç»­æ•™è‚²æœºæ„"]:
            return "æ‚¨å¥½ï¼Œç›®å‰æˆ‘ä»¬æ”¯æŒçš„ç”¨æˆ·ç±»å‹ä¸ºä¸“æŠ€ä¸ªäººï¼Œç”¨äººå•ä½ï¼Œä¸»ç®¡éƒ¨é—¨å’Œç»§ç»­æ•™è‚²æœºæ„ï¼Œè¯·ç¡®è®¤æ‚¨çš„ç”¨æˆ·ç±»å‹ã€‚"
        main_qa_agent_executor.agent.runnable.get_prompts()[0].template = (
            """Your ONLY job is to use a tool to answer the following question.

You MUST use a tool to answer the question. 
Simply Answer "æŠ±æ­‰ï¼Œæ ¹æ®æˆ‘çš„æœç´¢ç»“æœï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜" if you don't know the answer.
DO NOT answer the question without using a tool.

Current user role is """
            + user_role
            + """.

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

{chat_history}
Question: {input}
Thought:{agent_scratchpad}
"""
        )
        # st.session_state.user_role = user_role
        return f"æ›´æ–°æ‚¨çš„ç”¨æˆ·è§’è‰²ä¸º{user_role}, è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨ï¼Ÿ"


class CheckUserRoleTool(BaseTool):
    """æ ¹æ®ç”¨æˆ·å›ç­”ï¼Œæ£€æŸ¥ç”¨æˆ·è§’è‰²"""

    name: str = "æ£€æŸ¥ç”¨æˆ·è§’è‰²å·¥å…·"
    description: str = "ç”¨äºæ£€æŸ¥ç”¨æˆ·åœ¨å¯¹è¯ä¸­çš„è§’è‰²ï¼Œæ— éœ€è¾“å…¥å‚æ•° "
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    def _run(self, params) -> Any:
        print(params)
        template = main_qa_agent_executor.agent.runnable.get_prompts()[
            0
        ].template.lower()
        # print(template)
        start_index = template.find("current user role is") + len(
            "current user role is"
        )
        end_index = template.find("\n", start_index)
        result = template[start_index:end_index].strip()
        # result = st.session_state.get("user_role", "unknown")
        return result


class CheckUserLocTool(BaseTool):
    """æ ¹æ®ç”¨æˆ·å›ç­”ï¼Œæ£€æŸ¥ç”¨æˆ·å­¦ä¹ çš„åœ°å¸‚"""

    name: str = "æ£€æŸ¥ç”¨æˆ·åœ°å¸‚å·¥å…·"
    description: str = "ç”¨äºæ£€æŸ¥ç”¨æˆ·åœ°å¸‚ï¼Œæ— éœ€è¾“å…¥å‚æ•° "
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    def _run(self, params) -> Any:
        print(params)
        template = credit_problem_chain_executor.agent.runnable.get_prompts()[
            0
        ].template.lower()
        # print(template)
        start_index = template.find("user location: ") + len("user location: ")
        end_index = template.find("\n", start_index)
        result = template[start_index:end_index].strip()
        # result = st.session_state.get("user_role", "unknown")
        return result


class UpdateUserLocTool(BaseTool):
    """æ ¹æ®ç”¨æˆ·å›ç­”ï¼Œæ›´æ–°ç”¨æˆ·å­¦ä¹ åœ°å¸‚"""

    name: str = "ç”¨æˆ·å­¦ä¹ åœ°å¸‚æ›´æ–°å·¥å…·"
    description: str = (
        "ç”¨äºæ›´æ–°ç”¨æˆ·å­¦ä¹ åœ°å¸‚ï¼Œéœ€è¦æŒ‡é€šè¿‡ json æŒ‡å®šç”¨æˆ·å­¦ä¹ åœ°å¸‚ user_location "
    )
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    # def _run(self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None) -> Any:
    def _run(self, params) -> Any:
        print(params)
        try:
            params_dict = json.loads(params)
        except json.JSONDecodeError:
            return (
                "è¯·é—®æ‚¨æ˜¯åœ¨å“ªä¸ªåœ°å¸‚å¹³å°å­¦ä¹ çš„ï¼Ÿè¯·å…ˆç¡®è®¤æ‚¨çš„å­¦ä¹ åœ°å¸‚ï¼Œä»¥ä¾¿æˆ‘èƒ½ä¸ºæ‚¨æä¾›ç›¸åº”çš„ä¿¡æ¯ã€‚æˆ‘æ–¹è´Ÿè´£çš„ä¸»è¦å¹³å°åœ°å¸‚æœ‰ï¼š\n\n"
                + LOC_STR
            )
        if "user_location" not in params_dict:
            return (
                "è¯·é—®æ‚¨æ˜¯åœ¨å“ªä¸ªåœ°å¸‚å¹³å°å­¦ä¹ çš„ï¼Ÿè¯·å…ˆç¡®è®¤æ‚¨çš„å­¦ä¹ åœ°å¸‚ï¼Œä»¥ä¾¿æˆ‘èƒ½ä¸ºæ‚¨æä¾›ç›¸åº”çš„ä¿¡æ¯ã€‚æˆ‘æ–¹è´Ÿè´£çš„ä¸»è¦å¹³å°åœ°å¸‚æœ‰ï¼š\n\n"
                + LOC_STR
            )
        user_location = params_dict["user_location"]
        # if user_location not in LOC_STR:
        #     return "è¯·é—®æ‚¨æ˜¯åœ¨å“ªä¸ªåœ°å¸‚å¹³å°å­¦ä¹ çš„ï¼Ÿè¯·å…ˆç¡®è®¤æ‚¨çš„å­¦ä¹ åœ°å¸‚ï¼Œä»¥ä¾¿æˆ‘èƒ½ä¸ºæ‚¨æä¾›ç›¸åº”çš„ä¿¡æ¯ã€‚æˆ‘æ–¹è´Ÿè´£çš„ä¸»è¦å¹³å°åœ°å¸‚æœ‰ï¼š\n\n" + LOC_STR
        credit_problem_chain_executor.agent.runnable.get_prompts()[0].template = (
            """Use a tool to answer the user's qustion.

You MUST use a tool and generate a response based on tool's output.
When user input a number longer than 6 digits, use it as user id number in the context for the tool.
When the user input a four-digit number, use it as year in the context for the tool.
DO NOT hallucinate!!!!
                                                     
user location: """
            + user_location
            + """
You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

{chat_history}
Question: {input}
Thought:{agent_scratchpad}
"""
        )
        # st.session_state.user_role = user_role
        return f"è°¢è°¢ï¼Œå·²ä¸ºæ‚¨æ›´æ–°æ‚¨çš„å­¦ä¹ åœ°å¸‚ä¸º{user_location}, ç°åœ¨è¯·æ‚¨æä¾›èº«ä»½è¯å·ç ï¼Œä»¥ä¾¿æˆ‘æŸ¥è¯¢æ‚¨çš„å­¦æ—¶çŠ¶æ€ã€‚"


class CheckUserCreditTool(BaseTool):
    """æ ¹æ®ç”¨æˆ·å›ç­”ï¼Œæ£€æŸ¥ç”¨æˆ·å­¦æ—¶çŠ¶æ€"""

    name: str = "æ£€æŸ¥ç”¨æˆ·å­¦æ—¶çŠ¶æ€å·¥å…·"
    description: str = (
        "ç”¨äºæ£€æŸ¥ç”¨æˆ·å­¦æ—¶çŠ¶æ€ï¼Œéœ€è¦æŒ‡é€šè¿‡ json æŒ‡å®šç”¨æˆ·èº«ä»½è¯å· user_id_numberã€ç”¨æˆ·æƒ³è¦æŸ¥è¯¢çš„å¹´ä»½ yearã€ç”¨æˆ·æƒ³è¦æŸ¥è¯¢çš„è¯¾ç¨‹ç±»å‹ course_type "
    )
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    def _run(self, params) -> Any:

        params = params.replace("'", '"')
        print(params, type(params))
        CONTEXT_PROMPT = "You must ask the human about {context}. Reply with schema #2."
        try:
            params_dict = json.loads(params)
        except json.JSONDecodeError as e:
            print(e)
            return "éº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨çš„èº«ä»½è¯å·ï¼Œæˆ‘è¿™è¾¹å¸®æ‚¨æŸ¥ä¸€ä¸‹"

        if "user_id_number" not in params_dict:
            return "éº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨çš„èº«ä»½è¯å·"
        if len(params_dict["user_id_number"]) < 2:
            return "èº«ä»½è¯å·ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œéº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨æ­£ç¡®çš„èº«ä»½è¯å·"
        if "year" not in params_dict:
            return "æ‚¨é—®çš„æ˜¯å“ªä¸ªå¹´åº¦çš„è¯¾ç¨‹ï¼Ÿå¦‚ï¼š2019å¹´"
        if len(str(params_dict["year"])) < 2:
            return "å¹´åº¦ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œéº»çƒ¦æ‚¨ç¡®è®¤ä½ çš„è¯¾ç¨‹å¹´åº¦ã€‚å¦‚ï¼š2019å¹´"
        if "course_type" not in params_dict:
            return "æ‚¨è¦æŸ¥è¯¢çš„æ˜¯å…¬éœ€è¯¾è¿˜æ˜¯ä¸“ä¸šè¯¾"
        if len(params_dict["course_type"]) < 2:
            return "è¯·ç¡®è®¤æ‚¨è¦æŸ¥è¯¢çš„æ˜¯å…¬éœ€è¯¾è¿˜æ˜¯ä¸“ä¸šè¯¾"

        user_id_number = str(params_dict["user_id_number"])
        year = re.search(r"\d+", str(params_dict["year"])).group()
        course_type = str(params_dict["course_type"])

        template = credit_problem_chain_executor.agent.runnable.get_prompts()[
            0
        ].template.lower()
        # print(template)
        start_index = template.find("user location: ") + len("user location: ")
        end_index = template.find("\n", start_index)
        user_provided_loc = template[start_index:end_index].strip()

        user_loc = REGISTRATION_STATUS[user_id_number]["æ³¨å†Œåœ°ç‚¹"]

        if user_provided_loc not in user_loc and user_loc not in user_provided_loc:
            if user_provided_loc in ["å¼€æ”¾å¤§å­¦","èŸ¹å£³äº‘å­¦","ä¸“æŠ€çŸ¥åˆ°","æ–‡æ—…å…","æ•™å¸ˆ"]:
                return f"ç»æŸ¥è¯¢æ‚¨æœ¬å¹³å°çš„å•ä½æ‰€åœ¨åŒºåŸŸæ˜¯{user_loc}ï¼Œä¸æ˜¯çœç›´ï¼Œéçœç›´å•ä½å­¦æ—¶æ— æ³•å¯¹æ¥ã€‚"
            return f"ç»æŸ¥è¯¢æ‚¨æœ¬å¹³å°çš„å•ä½æ‰€åœ¨åŒºåŸŸæ˜¯{user_loc}ï¼Œä¸æ˜¯{user_provided_loc}ï¼ŒåŒºåŸŸä¸ç¬¦å­¦æ—¶æ— æ³•å¯¹æ¥ï¼Œå»ºè®®æ‚¨å…ˆè¿›è¡Œâ€œå•ä½è°ƒè½¬â€,è°ƒè½¬åˆ°æ‚¨æ‰€åœ¨çš„åœ°å¸‚åï¼Œå†è”ç³»æ‚¨çš„å­¦ä¹ åŸ¹è®­å¹³å°ï¼Œæ¨é€å­¦æ—¶ã€‚"
        else:
            if user_provided_loc in ["å¼€æ”¾å¤§å­¦","èŸ¹å£³äº‘å­¦","ä¸“æŠ€çŸ¥åˆ°","æ–‡æ—…å…","æ•™å¸ˆ"]:
                return "è¯·å…ˆå’¨è¯¢æ‚¨å…·ä½“çš„å­¦ä¹ åŸ¹è®­å¹³å°ï¼Œå­¦æ—¶æ˜¯å¦æœ‰æ­£å¸¸æ¨é€è¿‡æ¥ï¼Œåªæœ‰æ¨é€äº†æˆ‘ä»¬æ‰èƒ½æ”¶åˆ°ï¼Œæ‰ä¼šæ˜¾ç¤ºå¯¹åº”å­¦æ—¶ã€‚"
            hours = CREDIT_HOURS.get(user_id_number)
            if hours is None:
                return "ç»æŸ¥è¯¢ï¼Œå¹³å°è¿˜æœªæ¥æ”¶åˆ°æ‚¨çš„ä»»ä½•å­¦æ—¶ä¿¡æ¯ï¼Œå»ºè®®æ‚¨å…ˆå’¨è¯¢æ‚¨çš„å­¦ä¹ åŸ¹è®­å¹³å°ï¼Œå­¦æ—¶æ˜¯å¦å…¨éƒ¨æ¨é€ï¼Œå¦‚æœå·²ç¡®å®šæœ‰æ¨é€ï¼Œè¯·æ‚¨24å°æ—¶åŠæ—¶æŸ¥çœ‹å¯¹æ¥æƒ…å†µï¼›æ¯å¹´7æœˆè‡³9æœˆï¼Œå› å­¦æ—¶å¯¹æ¥æ•°æ®è¾ƒå¤§ï¼Œæ­¤é˜¶æ®µå»ºè®®1-3å¤©åŠæ—¶å…³æ³¨ã€‚"
            year_hours = hours.get(year)
            if year_hours is None:
                return f"ç»æŸ¥è¯¢ï¼Œå¹³å°è¿˜æœªæ¥æ”¶åˆ°æ‚¨åœ¨{year}å¹´åº¦çš„ä»»ä½•å­¦æ—¶ä¿¡æ¯ï¼Œå»ºè®®æ‚¨å…ˆå’¨è¯¢æ‚¨çš„å­¦ä¹ åŸ¹è®­å¹³å°ï¼Œå­¦æ—¶æ˜¯å¦å…¨éƒ¨æ¨é€ï¼Œå¦‚æœå·²ç¡®å®šæœ‰æ¨é€ï¼Œè¯·æ‚¨24å°æ—¶åŠæ—¶æŸ¥çœ‹å¯¹æ¥æƒ…å†µï¼›æ¯å¹´7æœˆè‡³9æœˆï¼Œå› å­¦æ—¶å¯¹æ¥æ•°æ®è¾ƒå¤§ï¼Œæ­¤é˜¶æ®µå»ºè®®1-3å¤©åŠæ—¶å…³æ³¨ã€‚"
            course_year_hours = year_hours.get(course_type)
            if course_year_hours is None:
                return f"ç»æŸ¥è¯¢ï¼Œå¹³å°è¿˜æœªæ¥æ”¶åˆ°æ‚¨åœ¨{year}å¹´åº¦{course_type}çš„å­¦æ—¶ä¿¡æ¯ï¼Œå»ºè®®æ‚¨å…ˆå’¨è¯¢æ‚¨çš„å­¦ä¹ åŸ¹è®­å¹³å°ï¼Œå­¦æ—¶æ˜¯å¦å…¨éƒ¨æ¨é€ï¼Œå¦‚æœå·²ç¡®å®šæœ‰æ¨é€ï¼Œè¯·æ‚¨24å°æ—¶åŠæ—¶æŸ¥çœ‹å¯¹æ¥æƒ…å†µï¼›æ¯å¹´7æœˆè‡³9æœˆï¼Œå› å­¦æ—¶å¯¹æ¥æ•°æ®è¾ƒå¤§ï¼Œæ­¤é˜¶æ®µå»ºè®®1-3å¤©åŠæ—¶å…³æ³¨ã€‚"
            if len(course_year_hours) == 0:
                return f"ç»æŸ¥è¯¢ï¼Œå¹³å°è¿˜æœªæ¥æ”¶åˆ°æ‚¨åœ¨{year}å¹´åº¦{course_type}çš„å­¦æ—¶ä¿¡æ¯ï¼Œå»ºè®®æ‚¨å…ˆå’¨è¯¢æ‚¨çš„å­¦ä¹ åŸ¹è®­å¹³å°ï¼Œå­¦æ—¶æ˜¯å¦å…¨éƒ¨æ¨é€ï¼Œå¦‚æœå·²ç¡®å®šæœ‰æ¨é€ï¼Œè¯·æ‚¨24å°æ—¶åŠæ—¶æŸ¥çœ‹å¯¹æ¥æƒ…å†µï¼›æ¯å¹´7æœˆè‡³9æœˆï¼Œå› å­¦æ—¶å¯¹æ¥æ•°æ®è¾ƒå¤§ï¼Œæ­¤é˜¶æ®µå»ºè®®1-3å¤©åŠæ—¶å…³æ³¨ã€‚"
            total_hours = sum([x["å­¦æ—¶"] for x in course_year_hours])
            finished_hours = sum([x["å­¦æ—¶"] for x in course_year_hours if x["è¿›åº¦"] == 100 and x["è€ƒæ ¸"] == "åˆæ ¼"])
            unfinished_courses = [f"{x['è¯¾ç¨‹åç§°']}å®Œæˆäº†{x['è¿›åº¦']}%" for x in course_year_hours if x["è¿›åº¦"] < 100]
            untested_courses = [x['è¯¾ç¨‹åç§°'] for x in course_year_hours if x["è€ƒæ ¸"] == "æœªå®Œæˆ"]
            unfinished_str = "  \n\n".join(unfinished_courses)
            untested_str = "  \n\n".join(untested_courses)

            res_str = f"ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨{year}å¹´åº¦{course_type}çš„å­¦æ—¶æƒ…å†µå¦‚ä¸‹ï¼š  \n\n"
            res_str += f"æ‚¨æŠ¥åçš„æ€»å­¦æ—¶ï¼š{total_hours}  \n\n"
            res_str += f"å·²å®Œæˆå­¦æ—¶ï¼š{finished_hours}  \n\n"
            res_str += f"å…¶ä¸­ï¼Œä»¥ä¸‹å‡ èŠ‚è¯¾è¿›åº¦è¿˜æ²¡æœ‰è¾¾åˆ°100%ï¼Œæ¯èŠ‚è¯¾è¿›åº¦çœ‹åˆ°100%åæ‰èƒ½è®¡å…¥å­¦æ—¶  \n\n"
            res_str += unfinished_str + "  \n\n"
            res_str += f"ä»¥ä¸‹å‡ èŠ‚è¯¾è¿˜æ²¡æœ‰å®Œæˆè€ƒè¯•ï¼Œè€ƒè¯•é€šè¿‡åæ‰èƒ½è®¡å…¥å­¦æ—¶  \n\n"
            res_str += untested_str + "  \n\n"
            return res_str


class RefundTool(BaseTool):
    """æ ¹æ®ç”¨æˆ·å›ç­”ï¼Œæ£€æŸ¥ç”¨æˆ·è´­ä¹°è¯¾ç¨‹è®°å½•"""

    name: str = "æ£€æŸ¥ç”¨æˆ·è´­ä¹°è¯¾ç¨‹è®°å½•å·¥å…·"
    description: str = (
        "ç”¨äºæ£€æŸ¥ç”¨æˆ·è´­ä¹°è¯¾ç¨‹è®°å½•ï¼Œéœ€è¦æŒ‡é€šè¿‡ json æŒ‡å®šç”¨æˆ·èº«ä»½è¯å· user_id_numberã€ç”¨æˆ·æƒ³è¦æŸ¥è¯¢çš„è¯¾ç¨‹å¹´ä»½ yearã€ç”¨æˆ·æƒ³è¦æŸ¥è¯¢çš„è¯¾ç¨‹åç§° course_name "
    )
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    def _run(self, params) -> Any:

        params = params.replace("'", '"')
        print(params, type(params))
        try:
            params_dict = json.loads(params)
        except json.JSONDecodeError as e:
            print(e)
            return "éº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨çš„èº«ä»½è¯å·ï¼Œæˆ‘è¿™è¾¹å¸®æ‚¨æŸ¥ä¸€ä¸‹"

        if "user_id_number" not in params_dict:
            return "éº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨çš„èº«ä»½è¯å·"
        if len(params_dict["user_id_number"]) < 2:
            return "èº«ä»½è¯å·ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œéº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨æ­£ç¡®çš„èº«ä»½è¯å·"
        if "year" not in params_dict:
            return "æ‚¨é—®çš„æ˜¯å“ªä¸ªå¹´åº¦çš„è¯¾ç¨‹ï¼Ÿå¦‚ï¼š2019å¹´"
        if len(params_dict["year"]) < 4:
            return "å¹´åº¦ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œéº»çƒ¦æ‚¨ç¡®è®¤ä½ çš„è¯¾ç¨‹å¹´åº¦ã€‚å¦‚ï¼š2019å¹´"
        if "course_name" not in params_dict:
            return "æ‚¨é—®çš„è¯¾ç¨‹åç§°æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ï¼šæ–°é—»ä¸“ä¸šè¯¾åŸ¹è®­ç­"
        if len(params_dict["course_name"]) < 2:
            return "è¯¾ç¨‹åç§°ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œè¯·æ‚¨æä¾›æ‚¨æƒ³è¦æŸ¥è¯¢çš„è¯¾ç¨‹çš„æ­£ç¡®åç§°ã€‚å¦‚ï¼šæ–°é—»ä¸“ä¸šè¯¾åŸ¹è®­ç­"

        user_id_number = params_dict["user_id_number"]

        year = params_dict["year"]
        year = re.search(r"\d+", year).group()

        course_name = params_dict["course_name"]
        if COURSE_PURCHASES.get(user_id_number) is not None:
            purchases = COURSE_PURCHASES.get(user_id_number)
            if year in purchases:
                if course_name in purchases[year]:
                    progress = purchases[year][course_name]["è¿›åº¦"]
                    if progress == 0:
                        return "ç»æŸ¥è¯¢æ‚¨çš„è¿™ä¸ªè¯¾ç¨‹æ²¡æœ‰å­¦ä¹ ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»å³ä¸Šæ–¹ã€æˆ‘çš„å­¦ä¹ ã€‘ï¼Œé€‰æ‹©ã€æˆ‘çš„è®¢å•ã€‘ï¼Œæ‰¾åˆ°å¯¹åº”è¯¾ç¨‹ç‚¹å‡»ã€ç”³è¯·å”®åã€‘ï¼Œè´¹ç”¨åœ¨1ä¸ªå·¥ä½œæ—¥ä¼šåŸè·¯é€€å›ã€‚"
                    return f"ç»æŸ¥è¯¢ï¼Œæ‚¨çš„è¯¾ç¨‹{course_name}å­¦ä¹ è¿›åº¦ä¸º{progress}%ï¼Œå¯ä»¥æŒ‰ç…§æœªå­¦çš„æ¯”ä¾‹é€€è´¹ï¼Œå¦‚éœ€é€€è´¹è¯·è”ç³»å¹³å°çš„äººå·¥çƒ­çº¿å®¢æœæˆ–è€…åœ¨çº¿å®¢æœè¿›è¡Œåé¦ˆã€‚"
                return f"ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨{year}å¹´åº¦ï¼Œæ²¡æœ‰è´­ä¹°{course_name}ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„è¯¾ç¨‹åç§°ã€å¹´åº¦ã€èº«ä»½è¯å·æ˜¯å¦æ­£ç¡®ã€‚"
            return f"ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨{year}å¹´åº¦ï¼Œæ²¡æœ‰è´­ä¹°{course_name}ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„è¯¾ç¨‹åç§°ã€å¹´åº¦ã€èº«ä»½è¯å·æ˜¯å¦æ­£ç¡®ã€‚"
        return f"ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨{year}å¹´åº¦ï¼Œæ²¡æœ‰è´­ä¹°{course_name}ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„è¯¾ç¨‹åç§°ã€å¹´åº¦ã€èº«ä»½è¯å·æ˜¯å¦æ­£ç¡®ã€‚"


class CheckPurchaseTool(BaseTool):
    """æ ¹æ®ç”¨æˆ·å›ç­”ï¼Œæ£€æŸ¥ç”¨æˆ·è´­ä¹°è¯¾ç¨‹è®°å½•"""

    name: str = "æ£€æŸ¥ç”¨æˆ·è´­ä¹°è¯¾ç¨‹è®°å½•å·¥å…·"
    description: str = (
        "ç”¨äºæ£€æŸ¥ç”¨æˆ·è´­ä¹°è¯¾ç¨‹è®°å½•ï¼Œéœ€è¦æŒ‡é€šè¿‡ json æŒ‡å®šç”¨æˆ·èº«ä»½è¯å· user_id_numberã€ç”¨æˆ·æƒ³è¦æŸ¥è¯¢çš„è¯¾ç¨‹å¹´ä»½ yearã€ç”¨æˆ·æƒ³è¦æŸ¥è¯¢çš„è¯¾ç¨‹åç§° course_name "
    )
    # args_schema: Type[BaseModel] = CalculatorInput
    return_direct: bool = True

    def _run(self, params) -> Any:

        params = params.replace("'", '"')
        print(params, type(params))
        try:
            params_dict = json.loads(params)
            params_dict = {k: str(v) for k, v in params_dict.items()}
        except json.JSONDecodeError as e:
            print(e)
            return "éº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨çš„èº«ä»½è¯å·ï¼Œæˆ‘è¿™è¾¹å¸®æ‚¨æŸ¥ä¸€ä¸‹"

        if "user_id_number" not in params_dict:
            return "éº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨çš„èº«ä»½è¯å·"
        if len(str(params_dict["user_id_number"])) < 2:
            return "èº«ä»½è¯å·ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œéº»çƒ¦æ‚¨æä¾›ä¸€ä¸‹æ‚¨æ­£ç¡®çš„èº«ä»½è¯å·"
        if "year" not in params_dict:
            return "æ‚¨é—®çš„æ˜¯å“ªä¸ªå¹´åº¦çš„è¯¾ç¨‹ï¼Ÿå¦‚ï¼š2019å¹´"
        if len(str(params_dict["year"])) < 4:
            return "å¹´åº¦ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œéº»çƒ¦æ‚¨ç¡®è®¤ä½ çš„è¯¾ç¨‹å¹´åº¦ã€‚å¦‚ï¼š2019å¹´"
        if "course_name" not in params_dict:
            return "æ‚¨é—®çš„è¯¾ç¨‹åç§°æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ï¼šæ–°é—»ä¸“ä¸šè¯¾åŸ¹è®­ç­"
        if len(params_dict["course_type"]) < 2:
            return "è¯¾ç¨‹åç§°ä¼¼ä¹ä¸å¤ªå¯¹ï¼Œè¯·æ‚¨æä¾›æ‚¨æƒ³è¦æŸ¥è¯¢çš„è¯¾ç¨‹çš„æ­£ç¡®åç§°ã€‚å¦‚ï¼šæ–°é—»ä¸“ä¸šè¯¾åŸ¹è®­ç­"

        user_id_number = params_dict["user_id_number"]

        year = params_dict["year"]
        year = re.search(r"\d+", year).group()

        course_name = params_dict["course_name"]
        if COURSE_PURCHASES.get(user_id_number) is not None:
            purchases = COURSE_PURCHASES.get(user_id_number)
            if year in purchases:
                if course_name in purchases[year]:
                    progress = purchases[year][course_name]["è¿›åº¦"]
                    if progress == 0:
                        return f"ç»æŸ¥è¯¢ï¼Œæ‚¨å·²ç»è´­ä¹°{year}å¹´åº¦çš„{course_name}ï¼Œè¯·å‰å¾€ä¸“ä¸šè¯¾å¹³å°ï¼Œç‚¹å‡»å³ä¸Šæ–¹ã€æˆ‘çš„å­¦ä¹ ã€‘æ‰¾åˆ°å¯¹åº”è¯¾ç¨‹ç›´æ¥å­¦ä¹ ã€‚"
                    return f"ç»æŸ¥è¯¢ï¼Œæ‚¨å·²ç»è´­ä¹°{year}å¹´åº¦çš„{course_name}ï¼Œæ‚¨çš„å­¦ä¹ è¿›åº¦ä¸º{progress}%ã€‚è¯·å‰å¾€ä¸“ä¸šè¯¾å¹³å°ï¼Œç‚¹å‡»å³ä¸Šæ–¹ã€æˆ‘çš„å­¦ä¹ ã€‘æ‰¾åˆ°å¯¹åº”è¯¾ç¨‹ç»§ç»­å­¦ä¹ ã€‚"
                return f"ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨{year}å¹´åº¦ï¼Œæ²¡æœ‰è´­ä¹°{course_name}ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„è¯¾ç¨‹åç§°ã€å¹´åº¦ã€èº«ä»½è¯å·æ˜¯å¦æ­£ç¡®ã€‚"
            return f"ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨{year}å¹´åº¦ï¼Œæ²¡æœ‰è´­ä¹°{course_name}ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„è¯¾ç¨‹åç§°ã€å¹´åº¦ã€èº«ä»½è¯å·æ˜¯å¦æ­£ç¡®ã€‚"
        return f"ç»æŸ¥è¯¢ï¼Œæ‚¨åœ¨{year}å¹´åº¦ï¼Œæ²¡æœ‰è´­ä¹°{course_name}ï¼Œè¯·æ‚¨ç¡®è®¤æ‚¨çš„è¯¾ç¨‹åç§°ã€å¹´åº¦ã€èº«ä»½è¯å·æ˜¯å¦æ­£ç¡®ã€‚"


@st.cache_resource
def create_retrieval_tool(
    markdown_path,
    tool_name,
    tool_description,
    chunk_size: int = 100,
    chunk_overlap: int = 30,
    separators: List[str] = None,
    search_kwargs: dict = None,
    return_retriever: bool = False,
    rerank: bool = False,
):
    # Load files
    loader = UnstructuredMarkdownLoader(markdown_path)
    docs = loader.load()
    print(docs)

    # Declare the embedding model
    embeddings = DashScopeEmbeddings(
        model="text-embedding-v2",
    )
    # embeddings = OpenAIEmbeddings()

    # Chunk the files
    # chunk_size = 100
    # chunk_overlap = 30

    if separators is not None:
        text_splitter = RecursiveCharacterTextSplitter(
            # text_splitter = CharacterTextSplitter(
            separators=["\n\n"],
            # separators=["\n\n", "\n"], is_separator_regex=True
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
    else:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
    documents = text_splitter.split_documents(docs)
    vector = FAISS.from_documents(documents, embeddings)
    # if "login_problems" in markdown_path:
    #     import ipdb

    #     ipdb.set_trace()
    # Create a retriever tool
    if search_kwargs is None:
        retriever = vector.as_retriever()
    else:
        retriever = vector.as_retriever(search_kwargs=search_kwargs)

    # if rerank:
    #     # compressor = CohereRerank()
    #     compressor = FlashrankRerank()
    #     retriever = ContextualCompressionRetriever(
    #         base_compressor=compressor, base_retriever=retriever
    #     )

    registration_tool = create_retriever_tool(
        retriever,
        name=tool_name,
        description=tool_description,
    )
    # import ipdb
    # ipdb.set_trace()
    if return_retriever:
        return registration_tool, retriever
    return registration_tool


# create registration retrievers
registration_tool = create_retrieval_tool(
    "./policies/registration/registration.md",
    "registration_engine",
    "æŸ¥è¯¢å¤§ä¼—äº‘å­¦å¹³å°æ³¨å†Œæµç¨‹ï¼Œæ¥å›ç­”å¦‚ä½•æ³¨å†Œçš„ç›¸å…³é—®é¢˜ï¼Œå¹¶è¿”å›ç»“æœ",
)

auditing_tool = create_retrieval_tool(
    "./policies/registration/auditing.md",
    "auditing_engine",
    "å›ç­”å…³äºæ³¨å†Œå®¡æ ¸çš„ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šè´¦å·æ€ä¹ˆåœ¨å®¡æ ¸ï¼Ÿå¦‚ä½•æŸ¥è¯¢å®¡æ ¸çŠ¶æ€ï¼Ÿ",
)

withdrawal_tool = create_retrieval_tool(
    "./policies/registration/withdrawal_and_modification.md",
    "withdrawal_engine",
    "å›ç­”å…³äºå¦‚ä½•æ’¤å›ã€ä¿®æ”¹ã€é©³å›æ³¨å†Œçš„é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ³¨å†Œåå¦‚ä½•æ’¤å›ï¼Ÿä¸“æŠ€ä¸ªäººèƒ½æ’¤å›å—ï¼Ÿæ€ä¹ˆæ’¤å›ä¸äº†ï¼Ÿ",
)

faq_personal_tool = create_retrieval_tool(
    "./policies/registration/professional_individual_reg_page_faq.md",
    "professional_individual_registration_faq_engine",
    "å›ç­”ä¸“æŠ€ä¸ªäººæ³¨å†Œé¡µé¢ç»†é¡¹ï¼Œä»¥åŠå¸¸è§é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šè¯ä»¶å·æç¤ºå·²å­˜åœ¨ï¼Œæ²¡æœ‰è‡ªå·±ä¸“ä¸šï¼Œå•ä½æ‰¾ä¸åˆ°ï¼Œæ²¡æœ‰å•ä½æ€ä¹ˆåŠï¼ŒèŒç§°ç³»åˆ—æ€ä¹ˆé€‰æ‹©",
)

faq_employing_unit_tool = create_retrieval_tool(
    "./policies/registration/employing_unit_reg_page_faq.md",
    "employing_unit_registration_faq_engine",
    "å›ç­”ç”¨äººå•ä½æ³¨å†Œé¡µé¢ç»†é¡¹ï¼Œä»¥åŠå¸¸è§é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šå•ä½æ€§è´¨å’Œçº§åˆ«æ€ä¹ˆé€‰ï¼Œå•ä½æ‰€å±è¡Œä¸šé€‰ä»€ä¹ˆï¼Œä¸»ç®¡éƒ¨é—¨æ€ä¹ˆé€‰/ä»€ä¹ˆæ„æ€ã€ä¸Šçº§å•ä½æ˜¯ä»€ä¹ˆæ„æ€/æ€ä¹ˆé€‰ï¼ŒåŒçº§äººç¤¾é€‰ä»€ä¹ˆï¼Œä¿¡æ¯é€‰é”™äº†æ€ä¹ˆåŠ",
)

faq_cont_edu_tool = create_retrieval_tool(
    "./policies/registration/continuing_edu_inst_reg_page_faq.md",
    "continuing_education_institute_registration_faq_engine",
    "å›ç­”ç»§ç»­æ•™è‚²æœºæ„æ³¨å†Œé¡µé¢ç»†é¡¹ï¼Œä»¥åŠå¸¸è§é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæœºæ„çº§åˆ«æ€ä¹ˆé€‰ã€ä»€ä¹ˆæ„æ€ï¼Œè¡Œä¸šä¸»ç®¡éƒ¨é—¨æ˜¯ä»€ä¹ˆæ„æ€ã€æ€ä¹ˆé€‰ï¼ŒåŒçº§äººç¤¾éƒ¨é—¨æ€ä¹ˆé€‰/åŒçº§äººç¤¾å‘¢ï¼Œé€‰é”™äº†æ€ä¹ˆåŠ/é€‰çš„ä¸å¯¹ä¼šæœ‰ä»€ä¹ˆå½±å“",
)

cannot_register_tool = create_retrieval_tool(
    "./policies/registration/cannot_register.md",
    "cannot_register_engine",
    "å›ç­”ç”¨æˆ·æ— æ³•æ³¨å†Œçš„ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ³¨å†Œä¸äº†æ€ä¹ˆåŠï¼Œæ³¨å†Œä¸ä¸Šæ€ä¹ˆåŠï¼Œæ³¨å†Œä¸äº†ï¼Œæ— æ³•æ³¨å†Œï¼Œæ³¨å†Œä¿å­˜ä»¥åä»€ä¹ˆååº”ä¹Ÿæ²¡æœ‰ï¼Œæ³¨å†Œæ²¡ååº”",
    search_kwargs={"k": 1},
)

login_problems_tool = create_retrieval_tool(
    "./policies/registration/login_problems.md",
    "login_problems_engine",
    "å›ç­”ç”¨æˆ·ç™»å½•é—®é¢˜çš„ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šç™»å½•ä¸äº†ã€æ— æ³•ç™»å½•ã€æ€ä¹ˆç™»å½•ä¸ä¸Š",
    search_kwargs={"k": 1},
    chunk_size=100,
    separators=["\n\n"],
)

login_problems_detail_tool = create_retrieval_tool(
    "./policies/registration/login_problems_details.md",
    "login_problems_detail_engine",
    "å›ç­”ç”¨æˆ·ç™»å½•é—®é¢˜çš„ç»†èŠ‚ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ²¡æœ‰æ»‘å—ï¼Œæ‰¾ä¸åˆ°æ»‘å—ï¼Œç™»å½•ä¸ºä»€ä¹ˆæç¤ºéªŒè¯å¤±è´¥ï¼Œå“ªé‡Œæœ‰æ»‘å—ï¼Œå¯†ç é”™è¯¯ï¼Œå¿˜è®°å¯†ç ï¼Œè´¦å·ä¸å­˜åœ¨ï¼Œç™»å½•æ˜¾ç¤ºå®¡æ ¸ä¸­",
    search_kwargs={"k": 1},
    chunk_size=100,
    separators=["\n\n"],
)

# TODO: Add more here
forgot_password_tool = create_retrieval_tool(
    "./policies/registration/forgot_password.md",
    "forgot_password_engine",
    "å›ç­”ç”¨æˆ·å¿˜è®°å¯†ç çš„ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šå¿˜è®°å¯†ç æ€ä¹ˆåŠï¼Œå¯†ç å¿˜è®°äº†ï¼Œæ‰¾å›å¯†ç ",
    # search_kwargs={"k": 1},
    # chunk_size=100,
    # separators=["\n\n"]
)

# create operation retrievers
individual_operation_tool = create_retrieval_tool(
    "./policies/operation/individual_operation.md",
    "individual_operation_engine",
    "å›ç­”ä¸“æŠ€ä¸ªäººå­¦æ—¶ã€å­¦æ—¶ç”³æŠ¥ã€ä¿®æ”¹å•ä½çš„ç³»ç»Ÿæ“ä½œç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ€ä¹ˆå­¦æ—¶ç”³æŠ¥ï¼Œå¦‚ä½•æäº¤å­¦æ—¶ï¼Œä¸ºä»€ä¹ˆä¸èƒ½å­¦æ—¶ç”³æŠ¥ï¼Œå­¦æ—¶ç”³æŠ¥ä¿¡æ¯å¤©å¡«é”™äº†æ€ä¹ˆåŠï¼Œå­¦æ—¶ä¿¡æ¯å¡«å¥½åæ— æ³•ä¿å­˜ï¼Œæˆ‘æ€ä¹ˆä¸èƒ½å­¦æ—¶ç”³æŠ¥ã€æˆ‘çš„è´¦å·é‡Œæ€ä¹ˆæ²¡æœ‰å­¦æ—¶ç”³æŠ¥ï¼Œè¯ä¹¦å’Œå‘æ˜ä¸“åˆ©èƒ½ç”³æŠ¥ã€æŠµæ‰£å¤šå°‘å­¦æ—¶ã€‚å†å¦‚ï¼šæ€ä¹ˆä¿®æ”¹å•ä½ï¼Œä¿®æ”¹å•ä½çš„è¯ï¼Œç°åœ¨å•ä½èƒ½çŸ¥é“å—ï¼Œç°åœ¨å•ä½å®¡æ ¸å—ï¼Œå•ä½è°ƒè½¬æç¤ºæœ‰å¾…å®¡æ ¸ä¿¡æ¯ï¼Œä¸èƒ½ä¿®æ”¹å•ä½ï¼Œå•ä½è°ƒè½¬ä¿¡æ¯å¡«é”™æ€ä¹ˆåŠï¼Œæ€ä¹ˆåˆ é™¤äººå‘˜ï¼Œç¦»èŒçš„äººå‘˜æ€ä¹ˆåŠï¼Œæ€ä¹ˆè°ƒåˆ°ä¸´æ—¶å•ä½",
    search_kwargs={"k": 10},
    chunk_size=100,
    separators=["\n\n"],
)

employing_unit_operation_tool = create_retrieval_tool(
    "./policies/operation/employing_unit_operation.md",
    "employing_unit_operation_engine",
    "å›ç­”ç”¨äººå•ä½å­¦æ—¶ç”³æŠ¥ã€æ³¨å†Œå®¡æ ¸ã€ä¿¡æ¯å˜æ›´ã€æ›´æ¢ç®¡ç†å‘˜ã€äººå‘˜ä¿¡æ¯æŸ¥è¯¢çš„ç³»ç»Ÿæ“ä½œç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šå•ä½æ€ä¹ˆå®¡æ ¸ï¼Œæ€ä¹ˆæŠŠäººå‘˜è°ƒå‡ºå•ä½ï¼Œäººå‘˜ç¦»èŒäº†æ€ä¹ˆè°ƒå‡ºå»ï¼Œå¦‚ä½•å®¡æ ¸äººå‘˜æäº¤çš„å­¦æ—¶ï¼Œå­¦æ—¶ç”³æŠ¥é”™äº†ï¼Œå•ä½ä¹Ÿå®¡æ ¸äº†æ€ä¹ˆåŠï¼Œæ€ä¹ˆé©³å›ï¼Œå­¦æ—¶ç”³æŠ¥é”™äº†ï¼Œå•ä½ä¹Ÿå®¡æ ¸äº†æ€ä¹ˆåŠï¼Œå•ä½åŸ¹è®­è®¡åˆ’ï¼Œæ€ä¹ˆæäº¤ã€å®¡æ ¸ï¼Œæ€ä¹ˆæ›´æ¢å•ä½è¶…çº§ç®¡ç†å‘˜ï¼Œå•ä½å¦‚ä½•å¢åŠ ç®¡ç†å‘˜ï¼Œå¦‚ä½•æŸ¥è¯¢å•ä½åä¸‹ä¸“æŠ€äººå‘˜ä¿¡æ¯",
    search_kwargs={"k": 10},
    chunk_size=100,
    separators=["\n\n"],
)

supervisory_department_operation_tool = create_retrieval_tool(
    "./policies/operation/supervisory_department_operation.md",
    "supervisory_department_operation_engine",
    "å›ç­”ä¸»ç®¡éƒ¨é—¨æ³¨å†Œå®¡æ ¸ã€ä¿¡æ¯å˜æ›´ã€ç»§ç»­æ•™è‚²æœºæ„å®¡æ ¸ã€å•ä½è°ƒè½¬å®¡æ ¸ã€å­¦æ—¶ç”³æŠ¥å®¡æ ¸ã€äººå‘˜ä¿¡æ¯æŸ¥è¯¢çš„ç³»ç»Ÿæ“ä½œç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šå¦‚ä½•å®¡æ ¸å•ä½æˆ–ä¸ªäººæ³¨å†Œä¿¡æ¯ã€äººå‘˜æˆ–ç”¨äººå•ä½ä¿¡æ¯å˜æ›´å®¡æ ¸ã€å¦‚ä½•å®¡æ ¸ç»§æ•™æœºæ„ä¿¡æ¯ã€äººå‘˜è°ƒå…¥å’Œå•ä½è°ƒè½¬å®¡æ ¸æ“ä½œã€å¦‚ä½•å®¡æ ¸ä¸“æŠ€äººå‘˜çš„å­¦æ—¶ã€å­¦æ—¶æŠ¥é”™äº†ï¼Œæ€ä¹ˆé©³å›ã€å­¦æ—¶ç”³æŠ¥é”™äº†ï¼Œä¹Ÿå®¡æ ¸é€šè¿‡äº†ï¼Œè¿˜èƒ½é©³å›å—ã€å¦‚ä½•æŸ¥è¯¢ä¸»ç®¡éƒ¨é—¨ä¸‹é¢å•ä½æƒ…å†µ",
    search_kwargs={"k": 10},
    chunk_size=100,
    separators=["\n\n"],
)

# create modify info retrievers
personal_modify_info_tool = create_retrieval_tool(
    "./policies/modify_info/professional_person_modify_info.md",
    "professional_person_modify_info_engine",
    "å›ç­”ä¸“æŠ€ä¸ªäººæ³¨å†Œä¿¡æ¯ä¿®æ”¹ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ€ä¹ˆæ”¹å§“åã€å§“åé”™äº†èƒ½æ”¹å—ã€æ€ä¹ˆæ”¹èº«ä»½è¯å·ç ï¼Œæ€ä¹ˆä¿®æ”¹å•ä½/å•ä½é”™äº†æ€ä¹ˆæ”¹ã€æ€ä¹ˆä¿®æ”¹å•ä½åŒºåŸŸã€å•ä½åŒºåŸŸé”™äº†æ€ä¹ˆæ”¹ï¼Œæ€ä¹ˆä¿®æ”¹æ‰‹æœºå·ã€æ‰‹æœºå·é”™äº†ï¼Œæ€ä¹ˆä¿®æ”¹ï¼Œæ€ä¹ˆä¿®æ”¹èŒç§°ã€èŒç§°æ¢äº†ï¼Œæ€ä¹ˆæ”¹",
    search_kwargs={"k": 5},
    chunk_size=200,
    separators=["\n\n"],
)

employ_supervise_modify_info_tool = create_retrieval_tool(
    "./policies/modify_info/employ_supervise_modify_info.md",
    "employ_supervise_modify_info_engine",
    "å›ç­”ç”¨äººå•ä½ã€ä¸»ç®¡éƒ¨é—¨æ³¨å†Œä¿¡æ¯ä¿®æ”¹ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ€ä¹ˆæ›´æ¢è¶…çº§ç®¡ç†å‘˜ã€ç®¡ç†å‘˜èƒ½æ›´æ¢å—ï¼Œä¿®æ”¹ç”¨äººå•ä½/ä¸»ç®¡éƒ¨é—¨è´¦å·æ‰‹æœºå·ã€é‚®ç®±ï¼Œæ€ä¹ˆä¿®æ”¹å•ä½åç§°ï¼Œæ€ä¹ˆä¿®æ”¹ç»Ÿä¸€ä¿¡ç”¨ä»£ç ï¼Œå¦‚ä½•æŸ¥è¯¢ä¸Šçº§éƒ¨é—¨ã€ä¸Šçº§éƒ¨é—¨ç®¡ç†å‘˜ä¿¡æ¯ï¼Œæ€ä¹ˆä¿®æ”¹å•ä½åŒºåŸŸã€æ³¨å†Œåœ°/å•ä½åœ°å€ï¼Œæ€ä¹ˆæ›´æ¢å•ä½ä¸Šçº§éƒ¨é—¨",
    search_kwargs={"k": 5},
    chunk_size=200,
    separators=["\n\n"],
)

cont_edu_modify_info_tool = create_retrieval_tool(
    "./policies/modify_info/cont_edu_modify_info.md",
    "cont_edu_modify_info_engine",
    "å›ç­”ç»§ç»­æ•™è‚²æœºæ„çš„æ³¨å†Œä¿¡æ¯ä¿®æ”¹ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ€ä¹ˆæ›´æ¢è¶…çº§ç®¡ç†å‘˜ã€ç®¡ç†å‘˜èƒ½æ›´æ¢å—ï¼Œä¿®æ”¹ç»§ç»­æ•™è‚²æœºæ„è´¦å·æ‰‹æœºå·ã€é‚®ç®±ï¼Œæ€ä¹ˆä¿®æ”¹å•ä½åç§°ï¼Œæ€ä¹ˆä¿®æ”¹ç»Ÿä¸€ä¿¡ç”¨ä»£ç ï¼Œå¦‚ä½•æŸ¥è¯¢ä¸Šçº§éƒ¨é—¨ã€ä¸Šçº§éƒ¨é—¨ç®¡ç†å‘˜ä¿¡æ¯ï¼Œæ€ä¹ˆä¿®æ”¹å•ä½åŒºåŸŸã€æ³¨å†Œåœ°/å•ä½åœ°å€ï¼Œæ€ä¹ˆæ›´æ¢å•ä½ä¸Šçº§éƒ¨é—¨",
    search_kwargs={"k": 7},
    chunk_size=400,
    separators=["\n\n"],
)

# complaints
complaints_tool = create_retrieval_tool(
    "./policies/complaints/complaints.md",
    "complaints_engine",
    "å›ç­”ç”¨æˆ·æŠ•è¯‰ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šå»ºè®®å¢åŠ äººå‘˜åˆ é™¤åŠŸèƒ½ï¼Œå»ºè®®å•ä½è´¦å·å¯ä»¥ä¸ä½¿ç”¨ç®¡ç†å‘˜èº«ä»½è¯å·ï¼Œå¯ä»¥è‡ªå·±è®¾ç½®ï¼Œæµ®åŠ¨å…¬å‘Šé£˜çš„å¤ªå¿«ã€é®æŒ¡ä¿¡æ¯ï¼Œå…³é—­æŒ‰é’®ä¸æ˜æ˜¾ï¼Œä¸æ–¹ä¾¿å…³é—­ï¼Œå®¢æœè”ç³»æ–¹å¼é®æŒ¡ä¿¡æ¯ï¼Œå»ºè®®è®¾ç½®å…³é—­æŒ‰é’®ï¼ŒæŸ¥è¯¢ç»Ÿè®¡çš„æ•°æ®ã€æ€ä¹ˆå¯¼å‡ºæ•°æ®ã€æ€ä¹ˆå¯¼å‡ºå•ä½æ‰€æœ‰äººçš„å­¦ä¹ æƒ…å†µçš„æ•°æ®ï¼Œé€€ä¼‘äººå‘˜çš„è´¦å·æ€ä¹ˆåŠã€é€€ä¼‘äººå‘˜æ€ä¹ˆè°ƒå‡ºæœ¬å•ä½ã€æ€ä¹ˆåˆ é™¤é€€ä¼‘äººå‘˜çš„è´¦å·ï¼Œä¸ºä»€ä¹ˆä¸èƒ½æ‰‹æœºç½‘é¡µç™»é™†ã€æ‰‹æœºç½‘é¡µä¸èƒ½ç™»å½•ã€é¡µé¢æ˜¾ç¤ºä¸å…¨ï¼Œå»ºè®®æ·»åŠ æ–°çš„ä¸“ä¸šçš„è¯¾ç¨‹ã€è¯¾ç¨‹é‡Œæ²¡æœ‰æˆ‘çš„ä¸“ä¸šï¼Œæœ‰æ²¡æœ‰è¯¾ä»¶ã€æ²¡æœ‰è¯¾ä»¶è®²è§£å—ï¼Œè¯¾ç¨‹ä¸èƒ½å€é€Ÿæ’­æ”¾ã€è§†é¢‘æ’­æ”¾å¤ªæ…¢äº†ï¼Œè´­ä¹°æ€ä¹ˆä¸èƒ½ä¸€èµ·æ”¯ä»˜ã€è¯¾ç¨‹æ€ä¹ˆä¸€å—ä¹°",
    search_kwargs={"k": 10},
    chunk_size=100,
    separators=["\n\n"],
)

# policy inquiry
policy_inquiry_tool = create_retrieval_tool(
    "./policies/policy_inquiry/policy_inquiry.md",
    "policy_inquiry_engine",
    "å›ç­”ç”¨æˆ·æ”¿ç­–å’¨è¯¢ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šèŒç§°è¯„å®¡ä»€ä¹ˆæ—¶å€™ã€èŒç§°è¯„å®¡æœ‰ä»€ä¹ˆè¦æ±‚ï¼Œæ–°ä¸€å¹´ç»§ç»­æ•™è‚²å­¦ä¹ æ—¶é—´ã€ä»€ä¹ˆæ—¶å€™èƒ½æŠ¥åå­¦ä¹ ã€å¾€å¹´çš„è¯¾ç¨‹è¿˜èƒ½è¡¥å­¦å—ã€æŠ¥åæ—¶é—´ï¼ŒæŠ¥èŒç§°æœ‰ä»€ä¹ˆè¦æ±‚ï¼Œæˆ‘éœ€è¦ç»§ç»­æ•™è‚²å—ã€æ¯å¹´éƒ½éœ€è¦ç»§ç»­æ•™è‚²å—ï¼Œä¸ºä»€ä¹ˆè¦ç»§ç»­æ•™è‚²",
    search_kwargs={"k": 5},
    chunk_size=100,
    separators=["\n\n"],
)

# other questions
other_questions_tool = create_retrieval_tool(
    "./policies/other_questions/other_questions.md",
    "other_questions_engine",
    "å›ç­”ç”¨æˆ·å…¶ä»–é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šä¼šè®¡äººå‘˜éœ€è¦å‡ å¹´ç»§ç»­æ•™è‚²ã€ä¼šè®¡äººå‘˜åœ¨å“ªé‡Œå­¦ä¹ ã€ä¼šè®¡äººå‘˜éœ€è¦å­¦ä¹ å…¬éœ€è¯¾å—ã€ä¼šè®¡çš„æ€ä¹ˆè¡¥å­¦ã€å«ç”ŸæŠ€æœ¯åœ¨å“ªé‡Œå­¦ä¹ ã€åŒ»æŠ¤äººå‘˜åœ¨å“ªé‡Œå­¦ä¹ ã€å«ç”ŸæŠ€æœ¯ä¸“ä¸šæ€ä¹ˆè¡¥å­¦ï¼Œ å¹³å°ä¸Šæ€ä¹ˆæ”¶è´¹ï¼Œçœç›´å•ä½å…¬éœ€è¯¾æ€ä¹ˆæ”¶è´¹ã€è¯¾ç¨‹æ²¡å­¦å®Œæ€ä¹ˆåŠã€æ€ä¹ˆå¼€å‘ç¥¨ï¼Œæœ‰å«å¥å§”çš„ç”µè¯å—ã€æœ‰äººç¤¾ç”µè¯å—ã€æœ‰ä¸»ç®¡éƒ¨é—¨ç”µè¯å—ã€äººç¤¾ç”µè¯æ˜¯å“ªä¸€ä¸ªã€èŒç§°è¯„å®¡éƒ¨é—¨ç”µè¯æ˜¯ä»€ä¹ˆï¼Œè¯„èŒç§°éœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Œè¯„èŒç§°éœ€è¦å­¦ä¹ å‡ å¹´ç»§ç»­æ•™è‚²ï¼Œæ€ä¹ˆå’Œè´µå¹³å°åˆä½œã€æƒ³å’Œä½ ä»¬åˆä½œï¼Œæ€ä¹ˆè”ç³»ï¼Œä¹°è¯¾æ”¶è´¹å—ã€å­¦ä¹ è¦äº¤è´¹å—ã€ä¸ºä»€ä¹ˆè¦æ”¶è´¹ã€èƒ½ä¾¿å®œå—ã€æœ‰ä¼˜æƒ å—ï¼Œæ€ä¹ˆæ³¨é”€è´¦å·ã€æˆ‘è¦æŠŠè´¦å·æ³¨é”€",
    search_kwargs={"k": 8},
    chunk_size=100,
    separators=["\n\n"],
)

# online learning and test
online_learning_and_tests_tool = create_retrieval_tool(
    "./policies/online_learning_and_tests/online_learning_and_tests.md",
    "online_learning_and_tests_engine",
    "å›ç­”ç”¨æˆ·å…³äºåœ¨çº¿å­¦ä¹ å’Œè€ƒè¯•çš„ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šå¦‚ä½•æŠ¥ç­ã€æ€ä¹ˆæŠ¥åå­¦ä¹ ã€å…¬éœ€è¯¾æ€ä¹ˆæŠ¥åã€ä¸“ä¸šè¯¾æ€ä¹ˆæŠ¥åï¼Œæµå®å¸‚é«˜çº§èŒä¸šå­¦æ ¡/å±±ä¸œç†å·¥èŒä¸šå­¦é™¢/å¾®å±±å¿äººæ°‘åŒ»é™¢æ€ä¹ˆæŠ¥åè¯¾ç¨‹ã€æ€ä¹ˆè¡¥å­¦ã€å­¦ä¹ æ ‡å‡†ã€å¹´åº¦å­¦ä¹ è¦æ±‚æ˜¯ä»€ä¹ˆã€å­¦ä¹ åˆ°ä»€ä¹ˆæ—¶å€™ã€ä»€ä¹ˆæ—¶é—´èƒ½å­¦ã€æ˜å¹´å­¦è¡Œå—ã€è¯¾ç¨‹æ²¡å­¦å®Œæ€ä¹ˆåŠã€ç”¨è€ƒè¯•å—ã€å¿…é¡»è€ƒè¯•å—ã€è€ƒè¯•å¤šå°‘åˆ†åˆæ ¼ã€è€ƒè¯•åˆ†æ•°çº¿æ˜¯å¤šå°‘ã€è€ƒè¯•æœ‰å‡ æ¬¡æœºä¼šã€æˆ‘çš„è€ƒè¯•åœ¨å“ªã€æ€ä¹ˆçœ‹è€ƒè¯•",
    search_kwargs={"k": 8},
    chunk_size=100,
    separators=["\n\n"],
)

payments_tool = create_retrieval_tool(
    "./policies/payments/payments.md",
    "payments_engine",
    "å›ç­”ç”¨æˆ·å…³äºæ”¯ä»˜çš„ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šå‘ç¥¨æ€ä¹ˆå¼€ã€èƒ½ä¸èƒ½é‡å¼€å‘ç¥¨ã€å‘ç¥¨é”™äº†èƒ½é‡å¼€å—ã€å‘ç¥¨åˆ—è¡¨åœ¨å“ªã€æ€ä¹ˆæ‰¾å‘ç¥¨ã€è¯¾ç¨‹æ˜¯æ€ä¹ˆæ”¶è´¹çš„ã€1å­¦æ—¶å¤šå°‘é’±ã€è¯¾ç¨‹ä»€ä¹ˆä»·æ ¼ã€è¯¾ç¨‹æŠ¥åæœ‰ä¼˜æƒ å—ã€èƒ½ä¾¿å®œå—ã€é›†ä½“ç¼´è´¹å®¡æ ¸ã€é›†ä½“ç¼´è´¹æ€ä¹ˆé€€æ¬¾ã€é›†ä½“ç¼´è´¹ç”¨é”™å¡æ”¯ä»˜",
    search_kwargs={"k": 8},
    chunk_size=100,
    separators=["\n\n"],
)

certificate_and_hours_tool = create_retrieval_tool(
    "./policies/certificate_and_hours/certificate_and_hours.md",
    "certificate_and_hours_engine",
    "å›ç­”ç”¨æˆ·å…³äºè¯ä¹¦å’Œå­¦æ—¶çš„ç›¸å…³é—®é¢˜ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚ï¼šæ€ä¹ˆä¸‹è½½è¯ä¹¦ã€æ€ä¹ˆæ‰“å°è¯ä¹¦ã€è¯ä¹¦æ‰“å°ã€æ²¡æœ‰è¯ä¹¦ã€ä¸ºä»€ä¹ˆæ‰“å°ä¸äº†è¯ä¹¦ï¼Œå…¬éœ€è¯¾è¾¾æ ‡æ˜¯å¤šå°‘ã€ä¸“ä¸šè¯¾è¾¾æ ‡æ˜¯å¤šå°‘ã€è¾¾æ ‡è¦æ±‚ã€è¾¾æ ‡æ˜¯ä»€ä¹ˆæ ‡å‡†ï¼Œå­¦æ—¶å¯¹æ¥åˆ°å“ªã€ä¼šå¯¹æ¥åˆ°ä¼šè®¡å¹³å°å—ã€ä¼šå¯¹æ¥åˆ°æµå—å¸‚/å¾·å·å¸‚/ä¸œè¥å¸‚å¹³å°å—ï¼Œä¼šè®¡ç½‘å­¦çš„å­¦æ—¶å¯ä»¥å¯¹æ¥åˆ°çœå¹³å°å—ã€åœ¨æ–‡æ—…å…å¹³å°å­¦ä¹ çš„ï¼Œå­¦æ—¶æ²¡å¯¹æ¥",
    search_kwargs={"k": 3},
    chunk_size=100,
    separators=["\n\n"],
)
    

# Create Agent
# model = Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3})
# model.model_name = "qwen-max"
# model.model_kwargs = {"temperature": 0.3}


tools = [
    # multiply,
    RegistrationStatusTool(),
    # AskForUserRoleTool(),
    UpdateUserRoleTool(),
    registration_tool,
    auditing_tool,
    withdrawal_tool,
    faq_personal_tool,
    faq_employing_unit_tool,
    faq_cont_edu_tool,
    cannot_register_tool,
    login_problems_tool,
    login_problems_detail_tool,
    forgot_password_tool,
    individual_operation_tool,
    employing_unit_operation_tool,
    supervisory_department_operation_tool,
    personal_modify_info_tool,
    employ_supervise_modify_info_tool,
    cont_edu_modify_info_tool,
    complaints_tool,
    policy_inquiry_tool,
    other_questions_tool,
    online_learning_and_tests_tool,
    payments_tool,
    certificate_and_hours_tool
]

# DO NOT hallucinate!!! You MUST use a tool to collect information to answer the questions!!! ALWAYS use a tool to answer a question if possible. Otherwise, you MUST ask the user for more information.
prompt = hub.pull("hwchase17/react")
prompt.template = """Your ONLY job is to use a tool to answer the following question.

You MUST use a tool to answer the question. 
Simply Answer "æŠ±æ­‰ï¼Œæ ¹æ®æˆ‘çš„æœç´¢ç»“æœï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜" if you don't know the answer.
DO NOT answer the question without using a tool.

Current user role is unknown.

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

{chat_history}
Question: {input}
Thought:{agent_scratchpad}
"""
prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
    "chat_history",
]

memory = ConversationBufferMemory(memory_key="chat_history", input_key="input")
agent = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}), tools, prompt
)
main_qa_agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, memory=memory, verbose=True, handle_parsing_errors=True
)


# create a router chain


# check user role agent
check_user_role_router_prompt = hub.pull("hwchase17/react")
check_user_role_router_prompt.template = """Your ONLY job is to determine the user role. DO NOT Answer the question.

You MUST use a tool to find out the user role.
DO NOT hallucinate!!!!

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you will not answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!


Question: {input}
Thought:{agent_scratchpad}
user role:
"""
check_user_role_router_prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
]

check_user_role_router_tools = [CheckUserRoleTool()]

check_user_role_router_chain = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    # ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.3),
    check_user_role_router_tools,
    check_user_role_router_prompt,
)
check_user_role_router_chain_executor = AgentExecutor.from_agent_and_tools(
    agent=check_user_role_router_chain,
    tools=check_user_role_router_tools,
    verbose=True,
    handle_parsing_errors=True,
)

# Update user role agent
update_user_role_prompt = hub.pull("hwchase17/react")
update_user_role_prompt.template = """Your ONLY job is to ask the user to provide their role information regardless of the input.

You MUST ALWAYS say: è¯·é—®æ‚¨æ˜¯ä¸“æŠ€ä¸ªäººã€ç”¨äººå•ä½ã€ä¸»ç®¡éƒ¨é—¨ï¼Œè¿˜æ˜¯ç»§ç»­æ•™è‚²æœºæ„ï¼Ÿè¯·å…ˆç¡®è®¤æ‚¨çš„ç”¨æˆ·ç±»å‹ï¼Œä»¥ä¾¿æˆ‘èƒ½ä¸ºæ‚¨æä¾›ç›¸åº”çš„ä¿¡æ¯ã€‚
You MUST use a tool to update user role.
DO NOT hallucinate!!!!

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!


Question: {input}
Thought:{agent_scratchpad}
"""
update_user_role_prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
]

update_user_role_tools = [UpdateUserRoleTool()]

update_user_role_chain = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    # ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.3),
    update_user_role_tools,
    update_user_role_prompt,
)
update_user_role_chain_executor = AgentExecutor.from_agent_and_tools(
    agent=update_user_role_chain,
    tools=update_user_role_tools,
    verbose=True,
    handle_parsing_errors=True,
)


# routing
def check_user_role_and_route(info):
    print(info["topic"])
    if "unknown" in info["topic"]["output"].lower():
        return update_user_role_chain_executor
    return main_qa_agent_executor


main_qa_chain = {
    "topic": check_user_role_router_chain_executor,
    "input": lambda x: x["input"],
} | RunnableLambda(check_user_role_and_route)


# check_is_credit_record_chain = check_is_credit_record_prompt | Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}) | StrOutputParser()

# this will be replaced with an agent
# general_chain = PromptTemplate.from_template("""Respond to the following user input:

# # Question: {input}
# # Answer:""") | Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3})

credit_problem_prompt = PromptTemplate.from_template(
    """Use a tool to answer the user's qustion.

You MUST use a tool and generate a response based on tool's output.
When user input a number longer than 6 digits, use it as user id number in the context for the tool.
When the user input a four-digit number, use it as year in the context for the tool.
DO NOT hallucinate!!!! DO NOT Assume any user inputs. ALWAYS ask the user for more information if needed.

user location: unknown

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

{chat_history}
Question: {input}
Thought:{agent_scratchpad}
"""
)
credit_problem_prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
    "chat_history",
]

credit_problem_tools = [CheckUserCreditTool()]
credit_problem_memory = ConversationBufferMemory(
    memory_key="chat_history", input_key="input"
)
credit_problem_chain = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    # ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.3),
    credit_problem_tools,
    credit_problem_prompt,
)
credit_problem_chain_executor = AgentExecutor.from_agent_and_tools(
    agent=credit_problem_chain,
    tools=credit_problem_tools,
    memory=credit_problem_memory,
    verbose=True,
    handle_parsing_errors=True,
)

# check user location
check_user_loc_router_prompt = hub.pull("hwchase17/react")
check_user_loc_router_prompt.template = """Your ONLY job is to determine the user location. DO NOT Answer the question.

NO MATTER WHAT, use a tool to find out the user location.
ALWAYS use a tool to check the user location.
You MUST use a tool to find out the user location.
DO NOT hallucinate!!!!

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you will not answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!


Question: {input}
Thought:{agent_scratchpad}
user role:
"""
check_user_loc_router_prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
]

check_user_loc_router_tools = [CheckUserLocTool()]

check_user_loc_router_chain = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    # ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.3),
    check_user_loc_router_tools,
    check_user_loc_router_prompt,
)
check_user_loc_router_chain_executor = AgentExecutor.from_agent_and_tools(
    agent=check_user_loc_router_chain,
    tools=check_user_loc_router_tools,
    verbose=True,
    handle_parsing_errors=True,
)

# update user location agent
update_user_location_prompt = hub.pull("hwchase17/react")
update_user_location_prompt.template = (
    """Your ONLY job is to ask the user to provide their location information regardless of the input.

You MUST ALWAYS say: è¯·é—®æ‚¨æ˜¯åœ¨å“ªä¸ªåœ°å¸‚å¹³å°å­¦ä¹ çš„ï¼Ÿè¯·å…ˆç¡®è®¤æ‚¨çš„å­¦ä¹ åœ°å¸‚ï¼Œä»¥ä¾¿æˆ‘èƒ½ä¸ºæ‚¨æä¾›ç›¸åº”çš„ä¿¡æ¯ã€‚æˆ‘æ–¹è´Ÿè´£çš„ä¸»è¦å¹³å°åœ°å¸‚æœ‰ï¼š\n\n"""
    + LOC_STR
    + """

You MUST use a tool to update user role.
DO NOT hallucinate!!!!

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!


Question: {input}
Thought:{agent_scratchpad}
"""
)
update_user_location_prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
]

update_user_location_tools = [UpdateUserLocTool()]

update_user_location_chain = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    # ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.3),
    update_user_location_tools,
    update_user_location_prompt,
)
update_user_location_chain_executor = AgentExecutor.from_agent_and_tools(
    agent=update_user_location_chain,
    tools=update_user_location_tools,
    verbose=True,
    handle_parsing_errors=True,
)


def check_user_loc_and_route(info):
    print(info["topic"])
    if "unknown" in info["topic"]["output"].lower():
        return update_user_location_chain_executor
    return credit_problem_chain_executor


main_credit_problem_chain = {
    "topic": check_user_loc_router_chain_executor,
    "input": lambda x: x["input"],
} | RunnableLambda(check_user_loc_and_route)

# course progress
course_progress_problems_prompt = PromptTemplate.from_template(
    """Answer the user's question step by step. Don't give the whole answer at once. Guide the user to the solution.

Always start with Step 1 below, DO NOT go to Step 2. Only execute Step 1 first. Do Not include the keyword `Step 1` or `Step 2` in your response.

Step 1. First check the user's learning method belongs to ç”µè„‘æµè§ˆå™¨ or æ‰‹æœºå¾®ä¿¡æ‰«ç 

Step 2. Based on the user's choice in Step 1,
If the user's learning method belongs to ç”µè„‘æµè§ˆå™¨ or æ‰‹æœºå¾®ä¿¡æ‰«ç , then say ç”µè„‘æµè§ˆå™¨è¯·ä¸è¦ä½¿ç”¨IEã€edgeç­‰è‡ªå¸¦æµè§ˆå™¨ï¼Œå¯ä»¥ä½¿ç”¨æœç‹—ã€è°·æ­Œã€360æµè§ˆå™¨æé€Ÿæ¨¡å¼ç­‰æµè§ˆå™¨è¯•è¯•ã€‚
Otherwise, say ç›®å‰æ”¯æŒçš„å­¦ä¹ æ–¹å¼æ˜¯ç”µè„‘æµè§ˆå™¨æˆ–è€…æ‰‹æœºå¾®ä¿¡æ‰«ç ä¸¤ç§ï¼Œå»ºè®®æ‚¨å†ä½¿ç”¨æ­£ç¡®çš„æ–¹å¼è¯•è¯•
If the user's used the right method but still has problems, then say å»ºè®®æ¸…é™¤æµè§ˆå™¨æˆ–è€…å¾®ä¿¡ç¼“å­˜å†è¯•è¯•
If the user used the right method and æ¸…é™¤äº†ç¼“å­˜, then sayï¼ŒæŠ±æ­‰ï¼Œæ‚¨çš„é—®é¢˜æ¶‰åŠåˆ°æµ‹è¯•ï¼Œå»ºè®®æ‚¨è”ç³»å¹³å°çš„äººå·¥çƒ­çº¿å®¢æœæˆ–è€…åœ¨çº¿å®¢æœè¿›è¡Œåé¦ˆ

{chat_history}
Question: {input}
"""
)
course_progress_problems_prompt.input_variables = [
    "input",
    "chat_history",
]
course_progress_problems_llm = Tongyi(
    model_name="qwen-max", model_kwargs={"temperature": 0.3}
)
course_progress_problems_memory = ConversationBufferMemory(
    memory_key="chat_history", input_key="input", return_messages=True
)
course_progress_problems_llm_chain = LLMChain(
    llm=course_progress_problems_llm,
    memory=course_progress_problems_memory,
    prompt=course_progress_problems_prompt,
    verbose=True,
    output_key="output",
)


# multiple login
multiple_login_prompt = PromptTemplate.from_template(
    """Answer the user's question step by step. Don't give the whole answer at once. Guide the user to the solution.

Always start with Step 1 below, DO NOT go to Step 2. Only execute Step 1 first. Do Not include the keyword `Step 1` or `Step 2` in your response.

Step 1
First check the user's learning method belongs to ç”µè„‘æµè§ˆå™¨ or æ‰‹æœºå¾®ä¿¡æ‰«ç 

Step 2
Based on the user's choice in Step 1,
If the user's learning method belongs to ç”µè„‘æµè§ˆå™¨ or æ‰‹æœºå¾®ä¿¡æ‰«ç , then say è¯·å‹¿ä½¿ç”¨ç”µè„‘å’Œæ‰‹æœºåŒæ—¶ç™»å½•è´¦å·å­¦ä¹ ï¼Œä¹Ÿä¸è¦ä½¿ç”¨ç”µè„‘æˆ–æ‰‹æœºåŒæ—¶ç™»å½•å¤šäººè´¦å·å­¦ä¹ ã€‚
If the user say æ²¡æœ‰ç™»å½•å¤šä¸ªè´¦å·/æ²¡æœ‰åŒæ—¶ç™»å½• etc., say å»ºè®®æ‚¨æ¸…é™¤ç”µè„‘æµè§ˆå™¨æˆ–æ‰‹æœºå¾®ä¿¡ç¼“å­˜ï¼Œå¹¶ä¿®æ”¹å¹³å°ç™»å½•å¯†ç åé‡æ–°ç™»å½•å­¦ä¹ è¯•è¯•ã€‚

{chat_history}
Question: {input}
"""
)
multiple_login_prompt.input_variables = [
    "input",
    "chat_history",
]
multiple_login_llm = Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3})
multiple_login_memory = ConversationBufferMemory(
    memory_key="chat_history", input_key="input", return_messages=True
)
multiple_login_llm_chain = LLMChain(
    llm=multiple_login_llm,
    memory=multiple_login_memory,
    prompt=multiple_login_prompt,
    verbose=True,
    output_key="output",
)


# how to register class
register_class_prompt = PromptTemplate.from_template(
    """Answer the user's question step by step. Don't give the whole answer at once. Guide the user to the solution.

Always start with Step 1 below, DO NOT go to Step 2. Only execute Step 1 first. Do Not include the keyword `Step 1` or `Step 2` in your response.

Step 1. First kindly ask the user whether they want to register å…¬éœ€è¯¾ or ä¸“ä¸šè¯¾

Step 2. Based on the user's choice in Step 1,
If the user wants å…¬éœ€è¯¾, then say é€‰æ‹©ã€æµå®èŒä¸šæŠ€æœ¯å­¦é™¢ã€‘è¿™ä¸ªå¹³å°ï¼Œè¿›å…¥ã€é€‰è¯¾ä¸­å¿ƒã€‘ï¼Œå…ˆé€‰æ‹©ã€åŸ¹è®­å¹´åº¦ã€‘ï¼Œå†é€‰æ‹©å¯¹åº”å¹´åº¦çš„è¯¾ç¨‹æŠ¥åå­¦ä¹ å°±å¯ä»¥ã€‚å¦‚æœæœ‰è€ƒè¯•ï¼Œéœ€è¦è€ƒè¯•é€šè¿‡åæ‰èƒ½è®¡å…¥å¯¹åº”å¹´åº¦çš„å­¦æ—¶ã€‚
If the user wants ä¸“ä¸šè¯¾, say é€‰æ‹©ã€æµå®èŒä¸šæŠ€æœ¯å­¦é™¢ã€‘è¿™ä¸ªå¹³å°ï¼Œè¿›å…¥ã€é€‰è¯¾ä¸­å¿ƒã€‘ï¼Œå…ˆé€‰æ‹©ã€åŸ¹è®­å¹´åº¦ã€‘ï¼Œå†é€‰æ‹©ä¸æ‚¨èŒç§°ä¸“ä¸šç›¸ç¬¦æˆ–è€…ç›¸å…³çš„è¯¾ç¨‹è¿›è¡ŒæŠ¥åï¼Œç¼´è´¹åå¯ä»¥å­¦ä¹ ã€‚ä¸“ä¸šè¯¾å­¦å®Œå°±å¯ä»¥è®¡å…¥å¯¹åº”å¹´åº¦çš„å­¦æ—¶ï¼Œæ— éœ€è€ƒè¯•ã€‚
If the user wants both, then say å¦‚æœè¦æŠ¥åå…¬éœ€è¯¾ï¼Œé€‰æ‹©ã€æµå®èŒä¸šæŠ€æœ¯å­¦é™¢ã€‘è¿™ä¸ªå¹³å°ï¼Œè¿›å…¥ã€é€‰è¯¾ä¸­å¿ƒã€‘ï¼Œå…ˆé€‰æ‹©ã€åŸ¹è®­å¹´åº¦ã€‘ï¼Œå†é€‰æ‹©å¯¹åº”å¹´åº¦çš„è¯¾ç¨‹æŠ¥åå­¦ä¹ å°±å¯ä»¥ã€‚å¦‚æœæœ‰è€ƒè¯•ï¼Œéœ€è¦è€ƒè¯•é€šè¿‡åæ‰èƒ½è®¡å…¥å¯¹åº”å¹´åº¦çš„å­¦æ—¶ã€‚å¦‚æœè¦æŠ¥åä¸“ä¸šè¯¾ï¼Œé€‰æ‹©ã€æµå®èŒä¸šæŠ€æœ¯å­¦é™¢ã€‘è¿™ä¸ªå¹³å°ï¼Œè¿›å…¥ã€é€‰è¯¾ä¸­å¿ƒã€‘ï¼Œå…ˆé€‰æ‹©ã€åŸ¹è®­å¹´åº¦ã€‘ï¼Œå†é€‰æ‹©ä¸æ‚¨èŒç§°ä¸“ä¸šç›¸ç¬¦æˆ–è€…ç›¸å…³çš„è¯¾ç¨‹è¿›è¡ŒæŠ¥åï¼Œç¼´è´¹åå¯ä»¥å­¦ä¹ ã€‚ä¸“ä¸šè¯¾å­¦å®Œå°±å¯ä»¥è®¡å…¥å¯¹åº”å¹´åº¦çš„å­¦æ—¶ï¼Œæ— éœ€è€ƒè¯•ã€‚
{chat_history}
Question: {input}
"""
)
register_class_prompt.input_variables = [
    "input",
    "chat_history",
]
register_class_llm = Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3})
register_class_memory = ConversationBufferMemory(
    memory_key="chat_history", input_key="input", return_messages=True
)
register_class_llm_chain = LLMChain(
    llm=register_class_llm,
    memory=register_class_memory,
    prompt=register_class_prompt,
    verbose=True,
    output_key="output",
)

# refund agent
refund_prompt = PromptTemplate.from_template(
    """Use a tool to answer the user's qustion.

Ask the user to provide èº«ä»½è¯å·ï¼Œin order to æŸ¥è¯¢è¯¾ç¨‹ä¿¡æ¯
You MUST use a tool and generate a response based on tool's output.

When user input a number longer than 6 digits, use it as user èº«ä»½è¯å· in the context for the tool.
DO NOT hallucinate!!!! 

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

{chat_history}
Question: {input}
Thought:{agent_scratchpad}
"""
)
refund_prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
    "chat_history",
]

refund_tools = [RefundTool()]
refund_memory = ConversationBufferMemory(memory_key="chat_history", input_key="input")
refund_chain = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    # ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.3),
    refund_tools,
    refund_prompt,
)
refund_chain_executor = AgentExecutor.from_agent_and_tools(
    agent=refund_chain,
    tools=refund_tools,
    memory=refund_memory,
    verbose=True,
    handle_parsing_errors=True,
)

# refund_course_not_started_prompt = PromptTemplate.from_template(
#     """Now that user has confirmed that they have not started the course, you should say the following:
# å¯¹äºæ²¡æœ‰å­¦ä¹ çš„è¯¾ç¨‹ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»å³ä¸Šæ–¹ã€æˆ‘çš„å­¦ä¹ ã€‘ï¼Œé€‰æ‹©ã€æˆ‘çš„è®¢å•ã€‘ï¼Œæ‰¾åˆ°å¯¹åº”è¯¾ç¨‹ç‚¹å‡»ã€ç”³è¯·å”®åã€‘ï¼Œè´¹ç”¨åœ¨1ä¸ªå·¥ä½œæ—¥ä¼šåŸè·¯é€€å›ã€‚

# DO NOT answer the question. Simply provide the above information.
                                                                
# Question: {input}

# Answer:"""
# )
# refund_course_not_started_prompt.input_variables = ["input"]
# refund_course_not_started_llm_chain = LLMChain(
#     llm=Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
#     prompt=refund_course_not_started_prompt,
#     verbose=True,
#     output_key="output",
# )

# refund_ask_if_started_prompt = PromptTemplate.from_template(
#     """You should say the following:
# æ‚¨è¦é€€è´¹çš„è¯¾ç¨‹å­¦ä¹ äº†å—ï¼Ÿ

# DO NOT answer the question. Simply provide the above information.
                                                                
# Question: {input}

# Answer:"""
# )
# refund_ask_if_started_prompt.input_variables = ["input"]
# refund_ask_if_started_llm_chain = LLMChain(
#     llm=Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
#     prompt=refund_ask_if_started_prompt,
#     verbose=True,
#     output_key="output",
# )

# refund_router_prompt = PromptTemplate.from_template(
#     """Based on the chat history only, classify whether the user `å­¦ä¹ äº†è¯¾ç¨‹` or `æ²¡æœ‰å­¦ä¹ è¯¾ç¨‹` or `ä¸çŸ¥é“å­¦æ²¡å­¦è¯¾ç¨‹` or `ç”¨æˆ·æœªæä¾›ä¿¡æ¯`.

# # Do not answer the question. Simply classify it as being related to `å­¦ä¹ äº†è¯¾ç¨‹` or `æ²¡æœ‰å­¦ä¹ è¯¾ç¨‹` or `ä¸çŸ¥é“å­¦æ²¡å­¦è¯¾ç¨‹` or `ç”¨æˆ·æœªæä¾›ä¿¡æ¯`.
# # Do not respond with anything other than `å­¦ä¹ äº†è¯¾ç¨‹` or `æ²¡æœ‰å­¦ä¹ è¯¾ç¨‹` or `ä¸çŸ¥é“å­¦æ²¡å­¦è¯¾ç¨‹` or `ç”¨æˆ·æœªæä¾›ä¿¡æ¯`.

# {chat_history}
# Question: {input}

# # Classification:"""
# )
# refund_router_prompt.input_variables = ["input", "chat_history"]
# refund_router_memory = ConversationBufferMemory(
#     memory_key="chat_history", input_key="input"
# )
# refund_router_llm_chain = LLMChain(
#     llm=Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
#     prompt=refund_router_prompt,
#     memory=refund_router_memory,
#     verbose=True,
# )


# def refund_route(info):
#     print(info)
#     if "å­¦ä¹ äº†è¯¾ç¨‹" in info["topic"]["text"]:
#         print("å­¦ä¹ äº†è¯¾ç¨‹")
#         return refund_chain_executor
#     if "æ²¡æœ‰å­¦ä¹ è¯¾ç¨‹" in info["topic"]["text"]:
#         print("æ²¡æœ‰å­¦ä¹ è¯¾ç¨‹")
#         return refund_course_not_started_llm_chain
#     if "ä¸çŸ¥é“å­¦æ²¡å­¦è¯¾ç¨‹" in info["topic"]["text"]:
#         print("ä¸çŸ¥é“å­¦æ²¡å­¦è¯¾ç¨‹")
#         return refund_chain_executor
#     if "ç”¨æˆ·æœªæä¾›ä¿¡æ¯" in info["topic"]["text"]:
#         print("ç”¨æˆ·æœªæä¾›ä¿¡æ¯")
#         return refund_ask_if_started_llm_chain


# refund_full_chain = {
#     "topic": refund_router_llm_chain,
#     "input": lambda x: x["input"],
# } | RunnableLambda(refund_route)

# cannot find course agent
cannot_find_course_prompt = PromptTemplate.from_template(
    """Use a tool to answer the user's qustion.

You MUST use a tool and generate a response based on tool's output.

When user input a number longer than 6 digits, use it as user id number in the context for the tool.
DO NOT hallucinate!!!!

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do.
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

{chat_history}
Question: {input}
Thought:{agent_scratchpad}
"""
)
cannot_find_course_prompt.input_variables = [
    "agent_scratchpad",
    "input",
    "tool_names",
    "tools",
    "chat_history",
]

cannot_find_course_tools = [CheckPurchaseTool()]
cannot_find_course_memory = ConversationBufferMemory(
    memory_key="chat_history", input_key="input"
)
cannot_find_course_chain = create_react_agent(
    Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    # ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.3),
    cannot_find_course_tools,
    cannot_find_course_prompt,
)
cannot_find_course_chain_executor = AgentExecutor.from_agent_and_tools(
    agent=cannot_find_course_chain,
    tools=cannot_find_course_tools,
    memory=cannot_find_course_memory,
    verbose=True,
    handle_parsing_errors=True,
)
# check if the question topic
# check_is_credit_record_prompt = PromptTemplate.from_template("""Given the user input AND chat history below, classify it as either being about `å­¦æ—¶æ²¡æ˜¾ç¤º, å­¦æ—¶æœ‰é—®é¢˜` or `other`

# # Do not answer the question. Simply classify it as being related to `å­¦æ—¶æ²¡æ˜¾ç¤º` or `å­¦æ—¶æœ‰é—®é¢˜` or `å­¦æ—¶ç”³æŠ¥` or `å­¦æ—¶å®¡æ ¸` or `other`.
# # Do not respond with anything other than `å­¦æ—¶æ²¡æ˜¾ç¤º` or `å­¦æ—¶æœ‰é—®é¢˜` or `å­¦æ—¶ç”³æŠ¥` or `å­¦æ—¶å®¡æ ¸` or `other`.

# {chat_history}
# Question: {input}

# # Classification:""")

template = """Given the user input AND chat history below, classify whether the user's topic being about `å­¦æ—¶æ²¡æ˜¾ç¤º` or `å­¦æ—¶æœ‰é—®é¢˜` or `å­¦æ—¶ç”³æŠ¥` or `å­¦æ—¶å®¡æ ¸` or `è¯¾ç¨‹è¿›åº¦` or `å¤šä¸ªè®¾å¤‡ï¼Œå…¶ä»–åœ°æ–¹ç™»å½•` or `è¯¾ç¨‹é€€æ¬¾é€€è´¹ï¼Œè¯¾ç¨‹ä¹°é”™äº†` or `è¯¾ç¨‹æ‰¾ä¸åˆ°ï¼Œè¯¾ç¨‹æ²¡æœ‰äº†` or `other`.

# Do not answer the question. Simply classify it as being related to `å­¦æ—¶æ²¡æ˜¾ç¤º` or `å­¦æ—¶æœ‰é—®é¢˜` or `å­¦æ—¶ç”³æŠ¥` or `å­¦æ—¶å®¡æ ¸` or `è¯¾ç¨‹è¿›åº¦` or `å¤šä¸ªè®¾å¤‡ï¼Œå…¶ä»–åœ°æ–¹ç™»å½•` or `è¯¾ç¨‹é€€æ¬¾é€€è´¹ï¼Œè¯¾ç¨‹ä¹°é”™äº†` or `è¯¾ç¨‹æ‰¾ä¸åˆ°ï¼Œè¯¾ç¨‹æ²¡æœ‰äº†` or `other`.
# Do not respond with anything other than `å­¦æ—¶æ²¡æ˜¾ç¤º` or `å­¦æ—¶æœ‰é—®é¢˜` or `å­¦æ—¶ç”³æŠ¥` or `å­¦æ—¶å®¡æ ¸` or `è¯¾ç¨‹è¿›åº¦` or `å¤šä¸ªè®¾å¤‡ï¼Œå…¶ä»–åœ°æ–¹ç™»å½•` or `è¯¾ç¨‹é€€æ¬¾é€€è´¹ï¼Œè¯¾ç¨‹ä¹°é”™äº†` or `è¯¾ç¨‹æ‰¾ä¸åˆ°ï¼Œè¯¾ç¨‹æ²¡æœ‰äº†` or `other`.

{chat_history}
Question: {input}

# Classification:"""

check_is_credit_record_prompt = PromptTemplate(
    input_variables=["input", "chat_history"],
    template=template,
)

check_is_credit_record_memory = ConversationBufferMemory(
    memory_key="chat_history", input_key="input"
)

check_is_credit_record_chain = LLMChain(
    llm=Tongyi(model_name="qwen-max", model_kwargs={"temperature": 0.3}),
    prompt=check_is_credit_record_prompt,
    memory=check_is_credit_record_memory,
    verbose=True,
)


def check_is_credit_record_router(info):
    print(info)
    if "å­¦æ—¶æ²¡æ˜¾ç¤º" in info["topic"]["text"]:
        print("å­¦æ—¶æ²¡æ˜¾ç¤º")
        return main_credit_problem_chain
    if "å­¦æ—¶æœ‰é—®é¢˜" in info["topic"]["text"]:
        print("å­¦æ—¶æœ‰é—®é¢˜")
        return main_credit_problem_chain
    if "å­¦æ—¶ç”³æŠ¥" in info["topic"]["text"]:
        print("å­¦æ—¶ç”³æŠ¥")
        return main_qa_chain
    if "å­¦æ—¶å®¡æ ¸" in info["topic"]["text"]:
        print("å­¦æ—¶å®¡æ ¸")
        return main_qa_chain
    if "other" in info["topic"]["text"]:
        print("other")
        return main_qa_chain
    if "è¯¾ç¨‹è¿›åº¦" in info["topic"]["text"]:
        print("è¯¾ç¨‹è¿›åº¦")
        return course_progress_problems_llm_chain
    if "å¤šä¸ªè®¾å¤‡ï¼Œå…¶ä»–åœ°æ–¹ç™»å½•" in info["topic"]["text"]:
        print("å¤šä¸ªè®¾å¤‡ï¼Œå…¶ä»–åœ°æ–¹ç™»å½•")
        return multiple_login_llm_chain
    if "è¯¾ç¨‹é€€æ¬¾é€€è´¹ï¼Œè¯¾ç¨‹ä¹°é”™äº†" in info["topic"]["text"]:
        print("è¯¾ç¨‹é€€æ¬¾é€€è´¹ï¼Œè¯¾ç¨‹ä¹°é”™äº†")
        return refund_chain_executor
        # return refund_full_chain
    if "è¯¾ç¨‹æ‰¾ä¸åˆ°ï¼Œè¯¾ç¨‹æ²¡æœ‰äº†" in info["topic"]["text"]:
        print("è¯¾ç¨‹æ‰¾ä¸åˆ°ï¼Œè¯¾ç¨‹æ²¡æœ‰äº†")
        return cannot_find_course_chain_executor
    print("unknown")
    return main_qa_chain


full_chain = {
    "topic": check_is_credit_record_chain,
    "input": lambda x: x["input"],
} | RunnableLambda(check_is_credit_record_router)


if "chat_engine" not in st.session_state.keys():  # Initialize the chat engine
    # st.session_state.chat_engine = index.as_chat_engine(
    #     chat_mode="condense_question", verbose=True
    # )
    st.session_state.chat_engine = full_chain

if prompt := st.chat_input(
    "æ‚¨çš„é—®é¢˜"
):  # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # st.session_state.chat_engine.memory.add_message(
    #     {"role": "user", "content": prompt}
    # )

for message in st.session_state.messages:  # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # response = st.session_state.chat_engine.chat(prompt)
            # response = st.session_state.chat_engine.invoke({"input": prompt})
            response = st.session_state.chat_engine.invoke({"input": prompt})
            print(response)
            # st.write(response)
            st.write(response["output"])
            message = {"role": "assistant", "content": response["output"]}
            # message = {"role": "assistant", "content": response}
            # st.session_state.chat_engine.memory.add_message(message)
            st.session_state.messages.append(message)  # Add response to message history
